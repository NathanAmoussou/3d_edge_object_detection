{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Edge Object Detection - Training Notebook\n",
    "\n",
    "This notebook implements the training pipeline for a custom object detector (SSDLite with MobileNetV3 backbone) on a Pascal VOC-like dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import (\n",
    "    SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    ssdlite320_mobilenet_v3_large,\n",
    ")\n",
    "from torchvision.models.detection import _utils as det_utils\n",
    "from torchvision.models.detection.ssdlite import SSDLiteHead\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luxonis/OpenVINO Compatibility Patches\n",
    "\n",
    "MobileNetV3 uses `HardSigmoid` and `HardSwish` activations that are not well supported by OpenVINO/Luxonis blob converter. We replace them with equivalent operations using `ReLU6` which converts cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HardSigmoidCompat(nn.Module):\n",
    "    \"\"\"Luxonis-compatible HardSigmoid: relu6(x + 3) / 6\"\"\"\n",
    "    def forward(self, x):\n",
    "        return F.relu6(x + 3.0) / 6.0\n",
    "\n",
    "\n",
    "class HardSwishCompat(nn.Module):\n",
    "    \"\"\"Luxonis-compatible HardSwish: x * HardSigmoid(x)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hs = HardSigmoidCompat()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.hs(x)\n",
    "\n",
    "\n",
    "def replace_hard_ops(module: nn.Module):\n",
    "    \"\"\"\n",
    "    Recursively replace nn.Hardsigmoid and nn.Hardswish with \n",
    "    Luxonis-compatible versions throughout the model.\n",
    "    \n",
    "    Call this BEFORE exporting to ONNX.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Hardsigmoid):\n",
    "            setattr(module, name, HardSigmoidCompat())\n",
    "        elif isinstance(child, nn.Hardswish):\n",
    "            setattr(module, name, HardSwishCompat())\n",
    "        else:\n",
    "            replace_hard_ops(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VOC_ROOT = \"finetuning_dataset\"\n",
    "CLASS_NAME = \"cube_rouge\"\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 2\n",
    "OUT_PATH = \"cube_ssdlite.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_image_ids(imageset_txt: Path):\n",
    "    with imageset_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "def _ensure_train_val_splits(voc_root: Path, val_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Create ImageSets/Main/train.txt and val.txt if missing.\n",
    "    Uses all .xml files found in Annotations/.\n",
    "    \"\"\"\n",
    "    main_dir = voc_root / \"ImageSets\" / \"Main\"\n",
    "    main_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_txt = main_dir / \"train.txt\"\n",
    "    val_txt = main_dir / \"val.txt\"\n",
    "\n",
    "    if train_txt.exists() and val_txt.exists():\n",
    "        return\n",
    "\n",
    "    ann_dir = voc_root / \"Annotations\"\n",
    "    xmls = sorted([p.stem for p in ann_dir.glob(\"*.xml\")])\n",
    "    if not xmls:\n",
    "        raise RuntimeError(f\"No XML found in {ann_dir}\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(xmls)\n",
    "    n_val = max(1, int(len(xmls) * val_ratio))\n",
    "    val_ids = xmls[:n_val]\n",
    "    train_ids = xmls[n_val:]\n",
    "\n",
    "    train_txt.write_text(\"\\n\".join(train_ids) + \"\\n\", encoding=\"utf-8\")\n",
    "    val_txt.write_text(\"\\n\".join(val_ids) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"[split] train={len(train_ids)} val={len(val_ids)} written to {main_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCCubeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads a VOC-like dataset:\n",
    "      voc_root/\n",
    "        Annotations/*.xml\n",
    "        JPEGImages/*.jpg|png\n",
    "        ImageSets/Main/train.txt (list of ids without extension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, voc_root: str, split: str, class_name: str = \"cube_rouge\"):\n",
    "        self.voc_root = Path(voc_root)\n",
    "        self.split = split\n",
    "        self.class_name = class_name\n",
    "\n",
    "        imageset = self.voc_root / \"ImageSets\" / \"Main\" / f\"{split}.txt\"\n",
    "        if not imageset.exists():\n",
    "            raise FileNotFoundError(f\"Split file not found: {imageset}\")\n",
    "\n",
    "        self.ids = _read_image_ids(imageset)\n",
    "\n",
    "        self.ann_dir = self.voc_root / \"Annotations\"\n",
    "        self.img_dir = self.voc_root / \"JPEGImages\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _find_image_path(self, image_id: str) -> Path:\n",
    "        # Try common extensions\n",
    "        for ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
    "            p = self.img_dir / f\"{image_id}{ext}\"\n",
    "            if p.exists():\n",
    "                return p\n",
    "        # fallback: search by prefix\n",
    "        matches = list(self.img_dir.glob(f\"{image_id}.*\"))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "        raise FileNotFoundError(\n",
    "            f\"Image not found for id={image_id} in {self.img_dir}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.ids[idx]\n",
    "        xml_path = self.ann_dir / f\"{image_id}.xml\"\n",
    "        img_path = self._find_image_path(image_id)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Parse Pascal VOC XML\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.findtext(\"name\", default=\"\").strip()\n",
    "            if name != self.class_name:\n",
    "                continue\n",
    "\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "\n",
    "            # Safety clamp\n",
    "            xmin = max(0.0, min(xmin, w - 1))\n",
    "            ymin = max(0.0, min(ymin, h - 1))\n",
    "            xmax = max(0.0, min(xmax, w - 1))\n",
    "            ymax = max(0.0, min(ymax, h - 1))\n",
    "\n",
    "            if xmax <= xmin or ymax <= ymin:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(1)  # 1 = cube (0 = background)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"iscrowd\": torch.zeros((labels.shape[0],), dtype=torch.int64),\n",
    "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "            if boxes.numel()\n",
    "            else torch.zeros((0,), dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "        # torchvision detection expects float tensors [0..1]\n",
    "        img_t = torchvision.transforms.functional.to_tensor(img)\n",
    "        return img_t, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Prepare Data\n",
    "voc_root = Path(VOC_ROOT)\n",
    "_ensure_train_val_splits(voc_root, val_ratio=0.2, seed=42)\n",
    "\n",
    "train_ds = VOCCubeDataset(str(voc_root), split=\"train\", class_name=CLASS_NAME)\n",
    "val_ds = VOCCubeDataset(str(voc_root), split=\"val\", class_name=CLASS_NAME)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=(DEVICE.type == \"cuda\"),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=(DEVICE.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=2):\n",
    "    # SSDLite 320 (MobileNetV3 backbone)\n",
    "    # Load COCO weights then replace head for num_classes=2\n",
    "    weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
    "    model = ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "    \n",
    "    # Fixed input size for ssdlite320\n",
    "    size = (320, 320)\n",
    "\n",
    "    # Retrieve real output channels from backbone\n",
    "    out_channels = det_utils.retrieve_out_channels(model.backbone, size)\n",
    "\n",
    "    # Anchors per feature-map\n",
    "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "    # Same norm_layer as used in ssdlite torchvision definition\n",
    "    norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n",
    "\n",
    "    # Replace complete head\n",
    "    model.head = SSDLiteHead(out_channels, num_anchors, num_classes, norm_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 2 classes = background + cube\n",
    "model = build_model(num_classes=2)\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bn(module: nn.Module):\n",
    "    # Set all BatchNorm to eval mode (uses running_mean/var, does not update)\n",
    "    if isinstance(module, nn.modules.batchnorm._BatchNorm):\n",
    "        module.eval()\n",
    "\n",
    "def train_one_epoch(model, optimizer, loader, device, epoch, print_every=50):\n",
    "    model.train()\n",
    "    model.apply(freeze_bn)  # <-- important\n",
    "    total = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(loader):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        if (step + 1) % print_every == 0:\n",
    "            print(\n",
    "                f\"[epoch {epoch}] step {step + 1}/{len(loader)} loss={loss.item():.4f} \"\n",
    "                + \" \".join([f\"{k}={v.item():.3f}\" for k, v in loss_dict.items()])\n",
    "            )\n",
    "\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model, loader, device):\n",
    "    model.train()  # keep train() so loss is calculated\n",
    "    model.apply(freeze_bn)\n",
    "    total = 0.0\n",
    "    for images, targets in loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        total += sum(loss_dict.values()).item()\n",
    "    return total / max(1, len(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=max(1, EPOCHS // 3), gamma=0.2\n",
    ")\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr = train_one_epoch(\n",
    "        model, optimizer, train_loader, DEVICE, epoch, print_every=10\n",
    "    )\n",
    "    vl = evaluate_loss(model, val_loader, DEVICE)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"[epoch {epoch}] train_loss={tr:.4f} val_loss={vl:.4f}\")\n",
    "\n",
    "    if vl < best_val:\n",
    "        best_val = vl\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"class_name\": CLASS_NAME,\n",
    "                \"num_classes\": 2,\n",
    "            },\n",
    "            OUT_PATH,\n",
    "        )\n",
    "        print(\n",
    "            f\"[save] best model saved: {OUT_PATH} (val_loss={best_val:.4f})\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "ckpt = torch.load(OUT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Inference settings\n",
    "score_thr = 0.55\n",
    "topk = 10\n",
    "\n",
    "# Find a test image (take one from validation set or specify path)\n",
    "# img_path = \"finetuning_dataset/JPEGImages/20251209_190950.jpg\" \n",
    "# If that specific image doesn't exist, let's pick one from the validation set\n",
    "if len(val_ds) > 0:\n",
    "    img_id = val_ds.ids[0]\n",
    "    img_path = val_ds._find_image_path(img_id)\n",
    "else:\n",
    "    print(\"No validation images found.\")\n",
    "    img_path = None\n",
    "\n",
    "if img_path:\n",
    "    print(f\"Testing on: {img_path}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = torchvision.transforms.functional.to_tensor(pil_img).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model([img_tensor])[0]\n",
    "\n",
    "    boxes = pred[\"boxes\"].detach().cpu().numpy()\n",
    "    scores = pred[\"scores\"].detach().cpu().numpy()\n",
    "\n",
    "    # Filter + topk\n",
    "    idx = np.where(scores >= score_thr)[0]\n",
    "    idx = idx[np.argsort(-scores[idx])][:topk]\n",
    "\n",
    "    # Visualization using Matplotlib\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    np_img = np.array(pil_img)\n",
    "    plt.imshow(np_img)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for i in idx:\n",
    "        x1, y1, x2, y2 = boxes[i]\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        \n",
    "        rect = plt.Rectangle((x1, y1), w, h, fill=False, edgecolor='green', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\"{scores[i]:.2f}\", bbox=dict(facecolor='green', alpha=0.5), color='white', fontsize=10)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping inference as no image was found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Export for Luxonis\n",
    "\n",
    "Export the trained model to ONNX format with Luxonis-compatible activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_PATH = \"cube_ssdlite_luxonis.onnx\"\n",
    "\n",
    "# Load best checkpoint\n",
    "ckpt = torch.load(OUT_PATH, map_location=\"cpu\")\n",
    "export_model = build_model(num_classes=2)\n",
    "export_model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "# Replace HardSigmoid/HardSwish with Luxonis-compatible versions\n",
    "replace_hard_ops(export_model)\n",
    "\n",
    "export_model.eval()\n",
    "\n",
    "# Dummy input (batch=1, 3 channels, 320x320)\n",
    "dummy_input = torch.randn(1, 3, 320, 320)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    opset_version=12,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"boxes\", \"scores\"],\n",
    "    dynamic_axes=None,  # Fixed size for edge deployment\n",
    ")\n",
    "\n",
    "print(f\"ONNX model exported to: {ONNX_PATH}\")\n",
    "print(\"Now convert with blobconverter:\")\n",
    "print(f\"  python -m blobconverter --onnx-model {ONNX_PATH} --shaves 6 --data-type FP16 --version 2022.1 --output-dir blobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m blobconverter \\\n",
    "#   --onnx-model cube_raw.onnx \\\n",
    "#   --shaves 6 \\\n",
    "#   --data-type FP16 \\\n",
    "#   --version 2022.1 \\\n",
    "#   --output-dir blobs \\\n",
    "#   --no-cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
