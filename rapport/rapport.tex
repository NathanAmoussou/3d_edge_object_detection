\documentclass[12pt,a4paper]{article}

% Encodage et polices
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{multirow}


% Packages essentiels
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{xurl}

% Mise en page
\usepackage{geometry}
\geometry{margin=25mm}

\usepackage{setspace}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% En-têtes et pieds de page
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Université Côte d'Azur}
\fancyhead[R]{\leftmark}
\fancyfoot[L]{Noé Florence Report}
\fancyfoot[C]{Page \thepage/23}
\fancyfoot[R]{\today}

\fancypagestyle{plain}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}
\usepackage{hyperref}
\usepackage[french]{cleveref}
\usepackage{booktabs} 
\usepackage{tabularx}
\usepackage{xltabular}

\begin{document}
% Titre et auteur
\title{}
\author{}
\date{}

\begin{titlepage}
    \centering
    \includegraphics[width=0.6\textwidth]{UCA-Logo-2niveaux-RVB.png}

    {\scshape\LARGE University Côte d'Azur \par}
    \vspace{1.5cm}

    \textsc{\Large Edge Computing et IA embarquée }\par
    \vspace{1.5cm}
    
    {\Huge\bfseries Optimisation de la détection d'objets embarquée pour système IOT-CPS : Approche YOLOv11  \par}
    \vspace{2cm}
    {\Large\itshape 
    \textbf{Author:} \\  Noé Florence, Nathan Amoussou, Louis  \par

    }
    \vspace{1cm}

        {\large \textbf{Professeur:} \\  Gerad Rocher\par}
    \vfill

    {\large \textbf{ } \\
 \par}
    \vfill
    {\large \textbf{} \\ \par}
   \vfill
    
    % Facultatif : Superviseur
    \vspace{0.8cm}
    
\end{titlepage}



% Appliquer le style 'plain' à la première page pour qu'elle n'ait pas d'en-têtes ni de pieds de page
\thispagestyle{plain}

% --- Contenu du Document ---
\newpage




\maketitle

\tableofcontents

% \section{Context of the project and objectives}

\section{Introduction}

% Rappel du projet et de son intérêt, optimiser modèles IA parce que puissance de calcul limitée sur de l'embarquée et parfois pas de possibilitée pratique d'utiliser un modèle dans le cloud, et aussi parce que la pratique d'optimiser des modèles via différents levier, sur le hardware et directement le modèle, est répandue et accessible aujourd'hui

% Dans ce projet décrit dans ce rapport on va prendre un modèle d'ia et une sélection de hardware (décrit dans section ...), appliquer différentes couches et combinaisons d'optimisations (décrit dans scetion ...), et évaluer les performances et différentes métriques de consommation et performance sur différents hardwares en s'assurant que ces comparaisons sont le plus fair possible (décrit dans ...) et conclure sur la ou les combinaisons d'optim les plus intéressantes par hardware.

L’essor de l'informatique embarquée (ville intelligente, robotique, véhicules autonomes, santé connectée, etc.) s’accompagne d’un besoin croissant de déployer des modèles d’intelligence artificielle (IA) au plus proche des capteurs et des actionneurs. % Vérifier terme informatique embarquée
Ce déplacement du \textit{cloud} vers l’\textit{edge} est motivé à la fois par des contraintes pratiques (connectivité intermittente, coûts de transfert, confidentialité) et par des exigences techniques (faible latence, robustesse en temps réel), tout en restant limité par les ressources disponibles sur l’embarqué (mémoire, puissance de calcul, énergie). % Expliquer ou remplacer edge et cloud
Dans ce contexte, l’optimisation de modèles---via des leviers agissant sur le modèle lui-même et sur l’exploitation du matériel---est devenue une pratique répandue et de plus en plus accessible grâce aux outils et écosystèmes actuels.

Dans cette logique, le projet présenté dans ce rapport consiste à, partant d’un modèle d’IA et d’une sélection de plateformes matérielles (décrites en \cref{sec:contexte}), appliquer plusieurs couches d’optimisations (décrites en \cref{sec:benchmarking,subsec:optimisations_appliquees}). 
Nous mesurons ensuite l’impact de ces choix sur des métriques de performance et de coût en veillant à rendre les comparaisons aussi équitables que possible via un protocole commun (décrit en \cref{subsec:protocole_de_benchmarking,sec:resultats}). 

L’objectif final de ce projet est d’identifier, pour chaque plateforme, les compromis les plus pertinents et de conclure sur la ou les combinaisons d’optimisations offrant le meilleur équilibre entre efficacité et qualité d’inférence, par exemple (\cref{sec:analyse_et_conclusion}). 

\section{Cas d'usage choisi} \label{sec:contexte} 
% TODO : Modifier le titre

\subsection{Architecture à optimiser} % TODO : Modifier le titre/utiliser un \paragraph?

% Comme modèle d'IA à optimiser, on a choisit un modèle de vision/détection, c'est un modèle d'IA, qui reçoit une image en entrée (ou une vidéo sous forme d'une succession continue d'image) et retourne la position d'un ou de plusieurs objets spécifiques détectées dans l'image, objets pour lesquels ce modèle à été entrainé spécifiquement. Le modèle spécifique c'est YOLOv11 d'ultralytic, un modèle publié en telle année, avec telles caractéristiques (taille, cas d'usage, recognition, etc.). On a choisit ce modèle parce que il est représentatif des modèles/cas d'usage utilisées en EDGE/Embarqué/IOT, et aussi parce que pour faire le pont avec nos nécessité pour le porjet d'IOT, on s'intéresse aux modèles de vision.

Comme architecture d'IA à optimiser, nous avons choisi un modèle de vision par ordinateur dédié à la détection d’objets. 
Ce type de réseau reçoit en entrée une image (ou une séquence d’images pour une vidéo) et produit en sortie un ensemble de prédictions décrivant où se trouvent les objets d’intérêt et de quel type ils sont : typiquement des boîtes englobantes (\textit{bounding boxes}), associées à une classe et à un score de confiance. 
Dans la suite, l’ensemble de nos optimisations viseront donc à accélérer et/ou alléger cette chaîne d’inférence tout en contrôlant la dégradation éventuelle de la qualité de détection.

Dans ce cadre, nous choisissons \href{https://docs.ultralytics.com/fr/models/yolo11/}{\textbf{YOLO11}}, une famille de détecteurs d’objets temps réel publiée par l’entreprise américaine Ultralytics en septembre 2024.
Cette gamme est conçue pour offrir un bon compromis entre précision, vitesse et efficacité et est explicitement pensée pour être déployée dans des environnements variés (du GPU aux dispositifs \textit{edge}).
Ce choix est motivé à la fois par le caractère très représentatif de YOLO pour des cas d’usage \textit{edge}/embarqué, et par l’adéquation avec nos besoins de projet centrés sur la vision.

%TODO : Justifier pourquoi on l'a choisit.

\subsection{Matériel}

% Matériels sur lesquels optimiser puis tester le modèle d'IA divisé en deux caté :
% 1) Réference, non-edge, non embarqué, pas notre cible principal, à titre indicatif et de comparaison avec edge
% - un GPU 4070 laptop, 8GB vRAM, 32GB RAM
% - Un CPU i9 14900 HX 32GB ram
% 2) Matériel edge pour notre cas d'usage
% - une Pi4 (avec son CPU, checker en ligne)
% - un nvidia jetson orin nano 4GB (on s'intéresse au GPU)
% - une luxonis OAK D Pro (on s'intéresse au VPU myriad accélérateur)

% À la suite du choix de l’architecture IA et afin d’évaluer l’effet des optimisations dans des conditions réalistes, nous testons l’inférence sur un ensemble de plateformes hétérogènes. 
% Nous distinguons d’abord deux machines de référence---qui ne constituent pas notre cible principale \textit{edge}/embarqué, mais servent de point de comparaison indicatif---: un PC portable équipé d’un GPU NVIDIA RTX 4070 (8 Go de VRAM) et de 32 Go de RAM (nommé \textit{4070} dans la suite de ce document), ainsi qu’un CPU Intel Core i9-14900HX (24 cœurs / 32 \textit{threads}, 32 Go de RAM, nommé ensuite \textit{i9}).

% Pour coller à notre cas d’usage \textit{edge}, nous retenons également trois dispositifs représentatifs de familles d’accélération différentes. 
% Le Raspberry Pi 4 (nommé ensuite \textit{Pi}) constitue une cible CPU-\textit{only} (Cortex-A72, quatre cœurs/\textit{threads}) où l’on cherche surtout à maîtriser les performances sous fortes contraintes. 
% Le NVIDIA Jetson Orin Nano 4GB (nommé ensuite \textit{Orin}) apporte un GPU embarqué (architecture Ampere, 512 cœurs CUDA et 16 Tensor Cores) et 4 Go de LPDDR5, ce qui en fait une plateforme adaptée à l’exploration d’optimisations tirant parti du calcul parallèle. 
% Enfin, la Luxonis OAK-D Pro (nommé ensuite \textit{Oak}) embarque un VPU Movidius Myriad X, typique d’une approche utilisant un accélérateur dédié pour exécuter des modèles de vision efficacement à faible consommation.

À la suite du choix de l’architecture IA et afin d’évaluer l’effet des optimisations dans des conditions réalistes, nous testons l’inférence sur un ensemble de plateformes hétérogènes.
Notre cible principale étant l’IA \textit{edge}/embarqué, nous décrivons d’abord trois dispositifs représentatifs de familles d’accélération différentes :

\begin{itemize}
    \item \textbf{Raspberry Pi 4 (\textit{Pi})} --- cible CPU-\textit{only} généraliste.
    Le Pi repose sur un SoC Broadcom BCM2711 intégrant un CPU ARM Cortex-A72 (4 cœurs) et une mémoire LPDDR4 partagée. 
    En l’absence d’accélérateur IA dédié, l’inférence est principalement limitée par le budget de calcul scalaire/SIMD du CPU, ainsi que par la bande passante mémoire et les effets thermiques.
    Dans la section résultats, cette plateforme sert à analyser les gains des optimisations dans un régime fortement contraint.
    % TODO : Vérifier cette dernière phrase.

    \item \textbf{NVIDIA Jetson Orin Nano 4GB (\textit{Orin})} --- focus GPU-\textit{first}.
    L’Orin Nano combine un CPU ARM Cortex-A78AE (6 cœurs) et un GPU NVIDIA Ampere (512 cœurs CUDA, 16 Tensor Cores), avec 4~Go de LPDDR5 (64-bit, $\sim$34~GB/s).
    Un GPU (\textit{Graphics Processing Unit}) est un processeur massivement parallèle, particulièrement efficace pour les opérations de type produits matriciels, ce qui se traduit en pratique par une accélération notable de l’inférence IA par rapport à un CPU généraliste lorsque le modèle est bien vectorisable.
    Dans ce projet, nous utilisons explicitement l’Orin comme une plateforme d’inférence accélérée GPU : le modèle est exécuté côté GPU via l’écosystème NVIDIA (CUDA/TensorRT), afin de tirer parti des Tensor Cores.
    Le CPU est principalement utilisé pour l’orchestration et une partie du pré/post-traitement, ce qui permet d’étudier séparément les goulots d’étranglement GPU et CPU + transferts.
    Les limites attendues sont la capacité mémoire (4~Go) et la bande passante mémoire lorsque la résolution ou les activations augmentent, ainsi que le coût CPU du pré/post-traitement si l’inférence GPU devient très rapide.
    % TODO : Vérifier cette dernière phrase.

    \item \textbf{Luxonis OAK-D Pro (\textit{Oak})} --- focus VPU-\textit{first}.
    L’OAK-D Pro est une caméra qui exécute une grande partie du pipeline (traitements de vision, profondeur stéréo et inférence) sur un VPU embarqué (plateforme DepthAI/RVC).
    Un VPU (\textit{Vision Processing Unit}) est un accélérateur spécialisé pour la vision, conçu pour exécuter efficacement des traitements d’imagerie et des réseaux de neurones \`a faible consommation, avec un meilleur compromis perf/W qu’un CPU.
    Cette approche favorise l’efficacité énergétique et réduit la charge du CPU hôte, mais impose des contraintes plus fortes de mémoire embarquée et de compatibilité opérateur (modèles à convertir/adapter).
    Le goulot d’étranglement peut provenir du VPU (capacité de calcul/mémoire, support d’opérateurs), mais aussi du \textit{dataflow} hôte $\leftrightarrow$ caméra (USB).
    % TODO : Vérifier cette dernière phrase.
\end{itemize}

En complément (hors cible principale \textit{edge}/embarqué), nous utilisons deux machines de référence comme points de comparaison indicatifs : (i) un PC portable \textit{4070} équipé d’un GPU \textbf{NVIDIA GeForce RTX 4070 Laptop} (8~Go de VRAM) et de 32~Go de RAM, et (ii) un PC portable \textit{i9} basé sur un \textbf{Intel Core i9-14900HX} (24 cœurs / 32 \textit{threads}, 32~Go de RAM).

\begin{table}[H]
    \centering
    \footnotesize
    \caption{Caractéristiques matérielles des plateformes et mise en relation avec les goulots d'étranglement observés.}
    \label{tab:hardware_specs_bottlenecks}
    
    % J'utilise p{...} pour la colonne Mémoire pour permettre les retours à la ligne
    \begin{tabularx}{\textwidth}{@{}l X X p{2.5cm} X@{}}
        \toprule
        \textbf{Plateforme} & \textbf{Processeur hôte} & \textbf{Accélérateur} & \textbf{Mémoire} & \textbf{\textit{Bottleneck}} \\
        \midrule
        
        % RPi 4
        \textbf{Pi} & 
        ARM Cortex-A72 \newline (4 cœurs) & 
        \textit{Aucun} (CPU-only) & 
        LPDDR4 \newline (Partagée) & SIMD saturé, latence élevée due à l'absence d'accélération dédiée. \\
        \addlinespace
        
        % Orin Nano
        \textbf{Orin} & 
        ARM Cortex-A78AE \newline (6 cœurs) & 
        GPU NVIDIA Ampere \newline (512 cœurs, 16 Tensor Cores) & 
        4 Go LPDDR5 \newline (Partagée) & 4 Go de mémoires et  coût du pré/post-traitement sur CPU. \\
        \addlinespace
        
        % OAK-D Pro
        \textbf{Oak} & 
        Déporté sur Hôte \newline (via USB) & 
        VPU Myriad X & 
        Embarquée \newline (VPU) & Latence de transfert USB hôte-caméra et capacité de calcul du VPU. \\
        \addlinespace
        
        % PC i9
        \textbf{i9} & 
        Intel Core i9-14900HX \newline (24 cœurs) & 
        \textit{Aucun} (CPU-only) & 
        32 Go RAM & Puissance brute élevée mais ratio perf/Watt défavorable par rapport au GPU. \\
        \addlinespace
        
        % PC 4070
        \textbf{4070} & 
        Intel Core i9 & 
        NVIDIA RTX 4070 \newline (Laptop, 8 Go VRAM) & 
        8 Go VRAM \newline (Dédiée) & Inférence très rapide, le goulot se déplace vers les transferts PCIe et le driver. \\
        
        \bottomrule
    \end{tabularx}
\end{table}

\section{Optimisations} \label{sec:benchmarking}

% À partir du modèle et des hardwares clarifiés au-dessus, on a établi une liste de métriques de mesure d'une part dans subsec..., et d'optimisation dans subsec...

\subsection{Contexte} \label{subsec:optimisations_appliquees}

% TODO : Parler du fallback CPU sur INT8

% Décrire bien les pipelines

% Sur le modèle de vision retenue, on a appliqué ces optimisations (parfois séquentiellement parfois parallèlement);
% - élagage (n, s, m)
% - quantification (fp32, fp16)
% - input resolution de l'image dans le modèle
% On a aussi des optims spéxifiques à chaque hardware
% - les différents runtimes en fonctino du hard
% - fusion de couche en fonction du rt
% - nb de shaves sur oak
% - optims bins et sparsity sur trt et int8
%  Ce qui nous fait un total de X variantes par hard/rt

Afin de structurer notre exploration, nous regroupons les optimisations en quatre familles correspondant à un découpage $2\times2$ :
(i) optimisations à la compilation vs (ii) optimisations au \textit{runtime}, et, dans chaque cas,
(iii) optimisations agnostiques du matériel vs (iv) optimisations spécifiques à une plateforme.
Intuitivement, la compilation regroupe tout ce qui transforme le modèle avant l’exécution (export, conversion de format, génération d’un moteur, choix de précision, etc.), tandis que le \textit{runtime} correspond aux réglages appliqués au moment de l’inférence/l'utilisation (p.\,ex. fusions de couches activées par le moteur, choix du niveau d’optimisation du graphe).
Dans notre protocole, nous nous appuyons sur trois moteurs d’exécution (de \textit{runtime}) :
\begin{enumerate}
\item \textbf{ONNX Runtime (ORT)} pour exécuter des modèles au format ONNX (format standard décrivant un modèle IA sous forme d'un graphe de calcul),
\item \textbf{TensorRT (TRT)} sur matériel NVIDIA (génération d’un modèle IA sous un format \textit{engine} optimisé pour matériel NVIDIA),
\item et le \textbf{\textit{runtime} DepthAI (DRT)} sur Oak (exécution d’un modèle compilé au format \texttt{.blob} pour le VPU MyriadX).
\end{enumerate}
Le \cref{tab:runtimes_par_plateforme} récapitule, pour chaque plateforme matérielle, le moteurs d’exécution (\emph{runtime}) retenus afin d’assurer une comparaison cohérente entre cibles.

\begin{table}[ht]
\centering
\begin{tabular}{l l}
\hline
\textbf{Matériel} & \textbf{Moteur} \\
\hline
i9 & ORT \\
4070 & ORT \\
Pi & ORT \\
Orin & TRT\\
Oak & DRT \\
\hline
\end{tabular}
\caption{Correspondance entre plateformes matérielles et moteurs d’exécution considérés.}
\label{tab:runtimes_par_plateforme}
\end{table}

\subsection{Optimisations à la compilation}

\subsubsection{Optimisations agnostiques matériel}

Notre base d’expériences repose sur trois leviers, indépendants de la plateforme, qui modifient directement la complexité et/ou le coût numérique du modèle :
\begin{itemize}
\item \textbf{Taille (élagage) du modèle} (variante YOLO) : \{\texttt{n}, \texttt{s}, \texttt{m}\}.
Une variante plus petite d'un modèle d'IA réduit typiquement le nombre de paramètres et d’opérations, donc la latence et la mémoire, au prix d’une précision potentiellement moindre.
Concrètement dans notre \textit{workflow}, nous utilisons directement les checkpoints officiels Ultralytics (\texttt{yolo11n.pt}, \texttt{yolo11s.pt}, \texttt{yolo11m.pt}) et nous exportons chaque variante séparément au format ONNX.
Ce levier est particulièrement critique sur les cibles \textit{edge} contraintes et devrait réduire aussi l’empreinte mémoire côté GPU, ce qui limite les saturations mémoire et stabilise la latence.
\item \textbf{Résolution d’entrée du modèle} : \{640, 512, 416, 320, 256\}.
Une résolution d'image en entrée du modèle plus faible diminue le coût de calcul (moins de pixels à traiter) et accélère l’inférence, mais peut dégrader la détection des petits objets.
Dans notre code, la résolution est fixée à l’export via l’argument \texttt{imgsz} (export ONNX non-dynamique), ce qui fige la forme du tenseur d’entrée et, par conséquent, les dimensions des tenseurs produits par la tête de détection ; au runtime, on applique le même \emph{letterbox} vers $(\texttt{imgsz},\texttt{imgsz})$ pour rester cohérent.
Sur les plateformes où la mémoire/bande passante est un facteur (LPDDR4/Pi, LPDDR5 4~Go/Orin) et/ou où le pipeline est serré (Oak), ce choix devrait réduire fortement la taille des activations et donc la pression mémoire et les transferts, souvent déterminants pour la latence \textit{end-to-end}.
\item \textbf{Quantification du modèle} : \{\texttt{FP32}, \texttt{FP16}\}.
Passer en \texttt{FP16} réduit la bande passante mémoire et peut accélérer l’exécution sur du matériel qui l’exploite bien, avec un impact généralement limité sur la qualité.
Concrètement, on pilote ce choix dès l’export Ultralytics via \texttt{half=True} (FP16) ou \texttt{half=False} (FP32), ce qui produit des modèles ONNX dont l’entrée (et les poids) sont en \texttt{float16} ou \texttt{float32} ; lors du benchmark, on détecte ce dtype et on alimente l’inférence avec des tenseurs du même type.
C’est surtout pertinent sur les plateformes GPU où TensorRT/CUDA peut exploiter des chemins FP16 optimisés (notamment via les Tensor Cores) et où la réduction de bande passante/VRAM aide directement à tenir dans des budgets mémoire serrés (ex.~4~Go sur Orin).
\end{itemize}
% Ces trois axes définissent $3\times 5\times 2 = 30$ variantes de base.

\subsubsection{Optimisations spécifiques matériel}

Certaines optimisations dépendent directement de l’accélérateur ciblé :
\begin{itemize}
\item \textbf{Oak --- nombre de SHAVEs} : \{4, 5, 6, 7, 8\}.
Les SHAVEs sont des processeurs vectoriels internes du VPU ; en allouer davantage pour l'inférence du modèle d'IA peut améliorer le débit de certaines opérations, au prix d’une contention possible avec d’autres traitements sur l’appareil.
Dans notre \textit{workflow}, on ne change pas les SHAVEs à l’exécution : on recompile le modèle pour l’OAK en générant un \texttt{.blob} via les outils DepthAI (conversion/compilation), en fixant le nombre de SHAVEs lors de la compilation (paramètre de type \texttt{-sh}/\texttt{numShaves}).
Côté matériel, cet hyperparamètre agit directement sur la part de ressources du VPU réservée au réseau : cela peut réduire la latence ou augmenter le débit sur Oak si le réseau est limité par le calcul vectoriel, mais l’effet n’est pas garanti monotone car il dépend aussi des autres blocs actifs (ISP, stéréo, transferts/queues hôte $\leftrightarrow$ caméra).

\item \textbf{Orin --- heuristique de \textit{build}} (sélection de tactiques) : \{\texttt{false}, \texttt{true}\}.
Activer ce mode demande à TensorRT d’utiliser une sélection heuristique de certaines tactiques afin de réduire le temps de génération de l’\textit{engine}, au prix potentiel d’un \textit{engine} légèrement moins performant qu’avec une recherche/profilage plus exhaustive.
Dans notre \textit{workflow}, ce choix est appliqué uniquement au moment de la construction de l’\textit{engine} TensorRT (compilation ONNX $\rightarrow$ \textit{engine}) en basculant l’option \texttt{heuristic} ON/OFF dans la configuration du \textit{builder} ; ensuite, l’\textit{engine} est réutilisé (cache) pour exécuter les mesures.
Côté matériel, cela n’accélère pas directement le GPU : l’objectif est surtout de raccourcir la phase de compilation et donc le \textit{time-to-first-run}. On s’attend donc à un impact faible (souvent nul) sur les performances \textit{runtime} une fois l’\textit{engine} construit, même si en théorie un choix heuristique peut conduire à des tactiques un peu moins optimales.

\item \textbf{Orin --- \textit{sparsity}} : \{\texttt{false}, \texttt{true}\}.
Lorsque le modèle et le matériel le permettent, activer la sparsity vise à tirer parti d’une accélération matérielle pour des poids structurés clairsemés.
Dans notre \textit{workflow}, on active/désactive ce mode au moment de la construction TensorRT (\texttt{--sparsity=enable}), sans modifier l’architecture à l’exécution.
Côté matériel, l’accélération est conditionnée à une sparsité structurée 2:4 exploitée par les Tensor Cores ; comme nos modèles YOLO ne sont pas entraînés/prunés pour respecter ce motif, activer la sparsité devrait être en grande partie ignoré (ou n’apporter qu’un gain négligeable) sur l’Orin.
\end{itemize}

\subsection{Optimisations au \textit{runtime} (agnostiques matériel)}

\subsubsection{Optimisations agnostiques matériel}

Nous n’avons pas retenu d’optimisations \textit{runtime} agnostiques des plateformes.

\subsubsection{Optimisations spécifiques plateforme}

Au moment de l’exécution, nous évaluons un réglage de la fusion du graphe via ORT :
\begin{itemize}
  \item \textbf{Pi, i9, 4070 --- Fusion de graphe} : \{\texttt{DISABLE\_ALL}, \texttt{ENABLE\_BASIC}, \texttt{ENABLE\_EXTENDED}, \texttt{ENABLE\_ALL}\}.
  Juste avant l'exécution d'un modèle, ORT applique un ensemble de passes sans changement sémantique sur le graphe ONNX (simplifications, \textit{constant folding}, élimination de nœuds redondants, fusions d’opérateurs compatibles...). Le but est de réduire le nombre d’opérations exécutées et les mouvements mémoire (moins de kernels/appels, meilleure localité), ce qui devrait diminuer la latence E2E.
  Le gain est en général plus marqué sur CPU, où réduire les allocations, copies et surcoûts d’exécution améliore directement les performances et la pression mémoire. Sur GPU via ORT, les fusions peuvent aussi aider en réduisant des lancements de kernels et des passages mémoire, mais l’effet peut être moins net si le goulot principal est ailleurs (pré/post-traitement CPU, transferts). Enfin, sur Orin en TensorRT EP, l’impact de ces optimisations ORT est typiquement limité car TensorRT applique déjà ses propres optimisations/fusions lors de la construction de l’engine.
\end{itemize}

\subsection{Récapitulatif}

Le \cref{tab:optimisations_synthese} récapitule l’ensemble des optimisations étudiées, en distinguant celles appliquées à la compilation et au \textit{runtime}, ainsi que leur caractère agnostique ou spécifique à une plateforme.

% Requiert: \usepackage{booktabs} \usepackage{array}
\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabular}{%
  >{\raggedright\arraybackslash}p{0.18\linewidth}
  >{\raggedright\arraybackslash}p{0.32\linewidth}
  >{\raggedright\arraybackslash}p{0.50\linewidth}}
\toprule
\textbf{Famille} & \textbf{Optimisations} & \textbf{Effet attendu (matériel)} \\
\midrule
\textbf{Agnostique / compilation} &
Élagage (\texttt{n,s,m}) \newline
Résolution (640, 512, 416, 320, 256) \newline
Quantification (\texttt{FP32}, \texttt{FP16}) &
$\downarrow$ calcul + $\downarrow$ activations/mémoire : gains forts sur Pi (CPU-\textit{only}) et Oak (VPU), et aide à tenir dans les budgets mémoire (ex. Orin 4~Go). \\
\midrule
\textbf{Spécifique / compilation} &
SHAVEs (Oak, 5 niveaux) \newline
\textit{Heuristic} (Orin, on/off) \newline
\textit{Sparsity} (Orin, on/off) &
Oak: ajuste les ressources VPU allouées au modèle IA (débit/latence), contention possible avec le reste du pipeline. \newline
Orin: \textit{heuristic} vise surtout $\downarrow$ temps de compilation (impact \textit{runtime} faible). \newline
\textit{Sparsity}: attendu faible/nul si le modèle n’est pas au motif compatible (ex. 2:4). \\
\midrule
\textbf{Spécifique / runtime} &
Fusions  \newline
(Pi, i9, 4070, 3 niveaux) &
$\downarrow$ \textit{overhead} (fusions, allocations/copies) : gains surtout sur CPU, parfois sur GPU via ORT. \\
\bottomrule
\end{tabular}

\caption{Synthèse des optimisations évaluées (portée/phase) et effet attendu selon la plateforme.}
\label{tab:optimisations_synthese}
\end{table}

Ces optimisations sont globalement combinables : on peut, pour une plateforme donnée, composer les variantes agnostiques compilation (taille/résolution/précision) avec les options spécifiques compilation quand elles existent (p. ex. SHAVEs sur Oak, options de compilation sur TRT). En revanche, les réglages \textit{runtime} dépendent du moteur d’exécution : la fusion ORT ne s’applique qu’avec ONNX Runtime, et les optimisations TensorRT (\textit{heuristic}, \textit{sparsity}) ne concernent que les modèles compilés en \textit{engine} TRT et exécutés via TRT. De plus, selon le matériel, le point de départ n’est pas identique ; le VPU de la Oak ne prend que des modèles au format FP16, et pour la Pi, nous n'avons considérés que les modèles de taille \texttt{n}, ceux de tailles supérieurs étant trop lourds et ralentissant beaucoup trops les \textit{benchmarks}.

À partir de cette grille d’optimisations et des contraintes propres à chaque plateforme, nous pouvons établir ci-dessous un récapitulatif du nombre de variantes effectivement évaluables par matériel.

\begin{itemize}
\item Pi : $1 \text{ taille} \times 5 \text{ résolutions} \times 2 \text{ quantification} \times 3 \text{ fusion} = 30$ variantes.
\item Oak : $3  \text{ tailles} \times 5  \text{ réso.} \times 1 \text{ quantif.}  \times 5 \text{ SHAVEs}  = 75$ variantes.
\item Orin : $3 \text{ tailles} \times 5 \text{ réso.} \times 2 \text{ quantif.} \times 2 \textit{ heuris.} \times2  \textit{ spars.} = 120$ variantes,
\item i9 : $3 \text{ tailles} \times 5 \text{ réso.} \times 2 \text{ quantif.} \times 3  \text{ fusions} = 90$ variantes.
\item 4070 : $3 \text{ tailles} \times 5 \text{ réso.} \times 2 \text{ quantif.} \times 3  \text{ fusions} = 90$ variantes.
\end{itemize}

Soit un total de 405 variantes.

\section{Métriques mesurées} \label{sec:metriques_mesurees}

% TODO : Développer et mieux expliquer nos choix

Après avoir défini les différentes optimisations (agnostiques et spécifiques) ainsi que les variantes effectivement testables sur chaque plateforme, l’étape suivante consiste à mesurer systématiquement les performances des couples (matériel, combinaison d’optimisations). 
Pour garder une lecture claire, nous regroupons les mesures en deux familles : (i) des métriques décrivant la performance et la qualité du modèle IA (\cref{subsec:mesures_de_performances}), et (ii) des métriques décrivant l’utilisation des ressources matérielles pendant l’inférence (CPU, mémoire, accélérateurs, etc., \cref{subsec:mesures_de_consommation}).

\subsection{Mesures de performances du modèle d'IA} \label{subsec:mesures_de_performances}

Pour garder une lecture simple et comparable entre plateformes, nous concentrons l’analyse sur deux métriques principales : (i) la latence d’inférence \textit{end-to-end} (E2E) et (ii) la qualité de détection via le mAP@50.

\begin{itemize}
  \item \textbf{Latence E2E (\emph{end-to-end}) par image.}
  La latence E2E correspond au temps total pour traiter une image, en incluant pré-traitement $\rightarrow$ inférence $\rightarrow$ post-traitement (redimensionnement/\textit{letterbox}, exécution du modèle, NMS/formatage des sorties).
  Nous avons retenu cette métrique car elle reflète la latence réellement perçue en application, et surtout parce qu’elle est mesurable de manière homogène sur tous les matériels : obtenir une décomposition fiable (pré/inférence/post) n’est pas toujours comparable d’un \textit{backend} à l’autre, et devient particulièrement délicat sur Oak (pipeline DepthAI asynchrone, files d’attente et transferts hôte $\leftrightarrow$ caméra).

  \item \textbf{Qualité de détection : mAP@50.}
  Le mAP@50 (\textit{mean Average Precision} à IoU=0.50) est une métrique standard pour évaluer un détecteur d’objets.
  Elle mesure la qualité globale en calculant, pour chaque classe, l’aire sous la courbe précision-rappel (AP), puis en moyennant sur les classes (mAP). Une détection est considérée correcte si la boîte prédite recouvre la vérité terrain avec une IoU $\ge 0.50$ ; ainsi, plus le modèle détecte les bonnes classes avec des boîtes bien localisées (et des scores permettant un bon compromis précision/rappel), plus le mAP@50 augmente.
\end{itemize}


\subsection{Mesures de consommation du matériel} \label{subsec:mesures_de_consommation}

En complément des métriques de performance (E2E et mAP@50), nous instrumentons l’exécution afin d’observer la pression sur le système et d’interpréter les latences mesurées.
Toutes les mesures sont échantillonnées périodiquement pendant le \textit{benchmark} (\textit{thread} de \textit{monitoring}, typiquement toutes les 500 à \SI{2000}{ms}) puis résumées (moyenne, p95). 
\begin{itemize}
    \item \textbf{Utilisation CPU / GPU (et limites côté VPU)} : On mesure la charge CPU (Pi, i9) au niveau du processus de \textit{benchmark} via \texttt{psutil} (\texttt{Process.cpu\_percent}), avec une version normalisée par le nombre de cœurs logiques lorsque disponible. 
    Côté 4070, nous lisons l’utilisation GPU (\%) et la VRAM (MB) via \texttt{nvidia-smi} (\texttt{--query-gpu=utilization.gpu,memory.used}).
    Sur Orin, nous n’avons pas \texttt{nvidia-smi} : nous utilisons \texttt{tegrastats} et extrayons \texttt{GR3D\_FREQ} (\%) et \texttt{EMC\_FREQ} (\%) comme proxy respectivement de l’activité GPU et de la pression mémoire/bande passante. 
    Sur Oak (VPU), nous n’avons pas obtenu de métrique robuste et comparable d’« utilisation VPU » : nous ne reportons donc pas de \% d’occupation VPU. En pratique, nous récupérons plutôt des indicateurs internes via \texttt{SystemLogger} (charge CPU embarquée \textit{Leon CSS}) et, lorsque disponible, un \emph{temps d’inférence VPU estimé} à partir des traces (\texttt{DEPTHAI\_LEVEL=trace} + extraction par motif).
    Sources d’incertitude : (i) échantillonnage discret (pics possibles entre deux mesures), (ii) \texttt{cpu\_percent} dépend de l’ordonnanceur et peut inclure du \textit{jitter} Python, (iii) \texttt{GR3D\_FREQ} / \texttt{EMC\_FREQ} reflètent des fréquences/états (DVFS) plutôt qu’un \% d’occupation « pur », (iv) côté Oak, les traces peuvent être absentes/incomplètes selon le pipeline et la verbosité activée.

    \item \textbf{RAM / VRAM} : Nous suivons (i) la mémoire RSS du processus de \textit{benchmark} (empreinte mémoire du programme) et (ii) la RAM système utilisée (pression mémoire globale) à nouveau via \texttt{psutil} (\texttt{Process.memory\_info().rss} et \texttt{virtual\_memory}).
    Sur Oak, nous récupérons des compteurs mémoire internes (\textit{DDR} et \textit{CMX}) via \texttt{SystemLogger}. 
    Sur 4070, la VRAM utilisée est lue via \texttt{nvidia-smi} en même temps que l’utilisation GPU.
    Sources d’incertitude : (i) la RSS dépend de l’allocateur Python et peut sur-estimer l’empreinte « utile » (fragmentation, caches), (ii) la RAM système utilisée inclut caches/buffers OS (donc pas directement imputable au seul benchmark), (iii) la VRAM mesurée par \texttt{nvidia-smi} correspond à de l’allocation effective (pas nécessairement à des activations « actives » à l’instant), (iv) sur mémoire partagée (Jetson), la frontière RAM/VRAM est moins nette. 

    \item \textbf{Puissance électrique} : La puissance électrique active (W) est mesurée externe au système via une prise connectée Tapo P110, insérée entre le secteur et l’alimentation du matériel testé (mesure « niveau système »).
    Dans le code, le module de \textit{monitoring} se connecte à la prise via la bibliothèque \texttt{tapo} (\texttt{ApiClient}) et interroge \texttt{get\_current\_power()} à intervalle fixe (par défaut toutes les \SI{2}{s}), pour ne pas surcharger l'API Tapo qui autorise difficilement une fréquence plus élevée, avec gestion d’échecs temporaires et désactivation si les lectures échouent.
    Lorsque nécessaire, nous convertissons la valeur renvoyée en watts via un facteur d’échelle configurable (\texttt{--tapo-power-scale}). 
    Sources d’incertitude : (i) mesure au niveau prise \(\Rightarrow\) inclut les pertes du bloc d’alimentation (rendement) et d’éventuels périphériques, (ii) résolution temporelle limitée (\textit{polling} \SI{2}{s}) \(\Rightarrow\) pics courts potentiellement lissés, (iii) latence réseau/Wi-Fi et appels asynchrones \(\Rightarrow\) valeurs manquantes possibles, (iv) sensibilité à l’état du système (processus de fond), ce qui motive une mesure \textit{idle} préalable dans le protocole. 

\subsection{Résumé}

Le \cref{tab:mesures_incertitudes} synthétise les différentes métriques instrumentées durant les \textit{benchmarks}, en précisant pour chacune la méthode de mesure utilisée (outil ou bibliothèque) ainsi que les principales limites et sources d’incertitude identifiées, afin de faciliter l’interprétation des résultats et la comparaison entre plateformes.

% Requiert: \usepackage{tabularx} \usepackage{booktabs} \usepackage{array}
\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\linewidth}{%
  >{\raggedright\arraybackslash}p{0.13\linewidth}
  >{\raggedright\arraybackslash}p{0.23\linewidth}
  >{\raggedright\arraybackslash}X}
\toprule
\textbf{Métrique} & \textbf{Méthode de mesure} & \textbf{Incertitudes / limites} \\
\midrule
CPU (\%) &
\texttt{psutil} (\textit{process cpu\_percent}) &
Échantillonnage discret (pics manqués), bruit OS (tâches de fond/interruptions), \textit{jitter} Python/ordonnanceur ; dépend du pas de mesure et de la normalisation par cœurs. \\

GPU (\%)&
PC: \texttt{nvidia-smi} ; Jetson: \texttt{tegrastats} (\texttt{GR3D\_FREQ}) &
Asynchronisme (kernels en rafales), DVFS (fréquence $\neq$ occupation), proxy Jetson (fréquence/état plutôt qu’un \% exact), charge influencée par pré/post CPU et transferts. \\

RAM (MB) &
\texttt{psutil} (RSS proc. + RAM système) &
RSS peut inclure fragmentation/caches ; RAM système inclut caches/buffers OS (pas imputable au seul \textit{benchmark}) ; comparabilité limitée sur mémoire partagée (Jetson). \\

VRAM (MB) &
PC: \texttt{nvidia-smi} ; Oak: DepthAI \texttt{SystemLogger} (DDR/CMX) &
\texttt{nvidia-smi} mesure de l’allocation (pas l’usage instantané) ; sur Oak, compteurs internes dépendants du pipeline et non directement comparables à une VRAM GPU ; valeurs peuvent varier avec buffering/queues. \\

Puissance (W) &
Prise Tapo P110 : polling via API (toutes les 2s) &
Mesure « au mur » (inclut pertes alim + périphériques), résolution temporelle faible (pics lissés), latence réseau/Wi-Fi et lectures manquantes ; dépend de l’état idle et du bruit de fond système. \\
\bottomrule
\end{tabularx}

\caption{Synthèse des mesures de consommation/ressources : méthode et limites principales.}
\label{tab:mesures_incertitudes}
\end{table}



\textbf{}

  % \item CPU (\%) : charge du processus lors du benchmark.
  % \item Mémoire RAM (MB) :
  % \begin{itemize}
  %   \item RSS du processus de benchmark (empreinte mémoire du programme),
  %   \item RAM système utilisée (pression mémoire globale), utile sur les plateformes à mémoire partagée (ex. Jetson).
  % \end{itemize}
  % \item Accélérateurs NVIDIA (RTX 4070 / Jetson Orin) :
  % \begin{itemize}
  %   \item Utilisation GPU (\%) et VRAM (MB) via \texttt{nvidia-smi} sur PC,
  %   \item GR3D\_FREQ (\%) et EMC\_FREQ (\%) via \texttt{tegrastats} sur Jetson, pour approcher la charge GPU et la pression mémoire/bande passante.
  % \end{itemize}
  % \item OAK-D (VPU Myriad) : récupération d’indicateurs internes côté caméra (charge CPU embarquée \textit{Leon CSS}, mémoire \textit{DDR/CMX}) et, lorsque disponible, un temps d’inférence VPU estimé à partir des traces (\textit{profiling}).
  % \item Consommation électrique (W) et énergie par inférence (J) : lorsque la mesure est disponible (p.\,ex. via un wattmètre externe), nous reportons la puissance instantanée et l’énergie moyenne par inférence, afin d’évaluer l’efficacité énergétique (métrique clé en embarqué).
\end{itemize}

% On a des métriques sur les performances du modèles, et des métriques sur l'utilisation du hardware.
% Pour le modèle, on mesure :
% - mAP
% - Precision/recall/F1
% - E2E runtime
% - device time
% - preprocess
% - inference
% - postprocess
% - p50/p...
% - FPS
% On a aussi des métriques côtés hardwares :
% - Utilisation de la puce
% - Utilisation de la RAM
% - température
% - conso élec

\section{Protocole de benchmarking} \label{subsec:protocole_de_benchmarking}
\subsection{Description}

% Après avoir défini l’espace des optimisations et des métriques relevées, l’enjeu devient de mesurer leurs effets de façon comparable d’une plateforme à l’autre. Nous appliquons un protocole identique sur chaque matériel, fondé sur trois précautions : (i) une mesure à vide sur quelques secondes, sans exécuter d’inférence, afin d’estimer la consommation et l’utilisation des ressources dues uniquement au système et aux processus en arrière-plan ; (ii) une phase de \textit{warmup} (inférences initiales non comptabilisées) pour stabiliser le pipeline d’exécution du modèle de vision ; (iii) la répétition de chaque expérience trois fois, afin de lisser l’aléatoire, puis l’agrégation des résultats (moyenne) lors de l’analyse.

% Une fois ces conditions réunies, chaque couple (\textit{hardware}, \textit{runtime}) exécute les mêmes tests d’inférence sur un jeu de données commun. Nous utilisons \texttt{coco128}, qui contient 128 images, suffisamment petit pour rester exécutable rapidement sur l’ensemble des plateformes, tout en étant assez grand pour obtenir des métriques exploitables. Pour chaque variante, l’inférence est effectuée sur l’ensemble des images et les métriques sont ensuite calculées et enregistrées au format CSV.

% Enfin, la consommation électrique instantanée est mesurée de manière externe à l’aide de la prise connectée Tapo. Concrètement, la prise est insérée entre la source secteur et l’alimentation du matériel testé, ce qui permet de relever une puissance en watts au niveau système.

Après avoir défini l’espace des optimisations et des métriques relevées, l’enjeu devient de mesurer leurs effets de façon comparable d’une plateforme à l’autre. Nous appliquons un protocole identique sur chaque matériel, fondé sur trois précautions visant à limiter les principales sources d’incertitude identifiées précédemment :

\begin{enumerate}
  \item \textbf{Mesure à vide (\textit{idle baseline}).}
  Avant toute inférence, nous mesurons pendant quelques secondes le système \emph{sans charge applicative}, afin d’estimer la consommation et l’utilisation des ressources dues uniquement à l’OS et aux processus en arrière-plan.
  Cette étape permet (i) de détecter d’éventuelles perturbations (tâches de fond, indexation, mises à jour) et (ii) de disposer d’un niveau de référence pour interpréter les mesures « en charge ».
  Elle est particulièrement utile pour la puissance mesurée au niveau prise (qui inclut le bruit de fond et les pertes d’alimentation) ainsi que pour les métriques CPU/RAM (sensibles aux caches/buffers OS).

  \item \textbf{Phase de \textit{warmup} (inférences non comptabilisées).}
  Nous exécutons un nombre fixe d’inférences initiales sans les comptabiliser (par défaut 10), afin de stabiliser le pipeline d’exécution.
  Cette phase vise à atténuer les effets de \emph{cold start} (allocations mémoire, initialisation des \textit{backends}, remplissage des caches, premières compilations/optimisations internes) et la montée en fréquence progressive des composants (DVFS), qui peuvent biaiser les premières itérations.
  L’effet est marqué sur les exécutions accélérées (GPU/TensorRT) où les lancements asynchrones et les buffers peuvent s’amortir après quelques itérations, et sur Oak où les files d’attente/pipelines \textit{device} $\leftrightarrow$ \textit{host} peuvent nécessiter un régime établi pour être représentatifs.

  \item \textbf{Répétitions (3 exécutions) et agrégation.}
  Chaque expérience (même couple \textit{hardware}/\textit{runtime} et même variante de modèle) est répétée trois fois, puis nous agrégeons les résultats lors de l’analyse.
  Cette pratique permet de lisser la variabilité \textit{run-to-run} due au bruit OS, au \textit{jitter} d’échantillonnage des capteurs (CPU/GPU) et, pour la puissance, au \textit{polling} relativement lent de la prise (toutes les 2 secondes, pouvant lisser des pics courts).
  Les répétitions sont aussi un moyen simple de mitiger les mesures aberrantes (transitoires réseau pour la prise, \textit{throttling} ponctuel, contention inattendue), qui pourraient autrement être sur-interprétées.
\end{enumerate}

Une fois ces conditions réunies, chaque couple (\textit{hardware}, \textit{runtime}) exécute les mêmes tests d’inférence sur un jeu de données commun. Nous utilisons \texttt{coco128}, qui contient 128 images, suffisamment petit pour rester exécutable rapidement sur l’ensemble des plateformes, tout en étant assez grand pour obtenir des métriques exploitables. Pour chaque variante, l’inférence est effectuée sur l’ensemble des images et les métriques sont ensuite calculées et enregistrées au format CSV.

% \subsection{Précautions}

% \begin{enumerate}
% \item \textbf{Mesure à vide} : Une mesure préliminaire de quelques secondes est effectuée sans exécuter d’inférence. Cela permet d’isoler la consommation et l'utilisation des ressources dues uniquement au système d'exploitation et aux processus en arrière-plan.
% \item \textbf{Phase de Warmup}: Une série d'inférences initiales est exécutée mais non comptabilisée. Cette étape sert à stabiliser le pipeline d’exécution (chargement des librairies, caches CPU/GPU) avant le début des mesures.
% \item \textbf{Répétition et lissage}: Chaque exécution est répétée trois fois intégralement. Les résultats présentés dans l'analyse correspondent à la moyenne agrégée de ces exécutions, ce qui permet de lisser les variations aléatoires (jitter)
% \end{enumerate}

% \subsection{Limites méthodologiques}

% Malgré les précautions prises lors de la réalisation de ce protocole, nous avons quand identifié des incertitudes liés à la chaîne de mesure. Cependant l'application de même protocole sur tout les hardwares ainsi que des précautions citées en section 5.2 (notamment la redondance des prises de valeur et du warmup) garantit la validités des comparaison relatives et la viabilité de nos résultat.


% \renewcommand{\arraystretch}{1.4} % Aère les lignes

% \begin{xltabular}{\textwidth}{@{} p{2.5cm} p{3cm} >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X @{}}

%     \caption{Tableau des Incertitudes et Limites du Protocole de Mesure} \\
%     \toprule
%     \textbf{Métrique Mesurée} & \textbf{Outil / Méthode} & \textbf{Précision / Marge d'erreur} & \textbf{Sources d'Incertitude et Biais} \\
%     \midrule
%     \endfirsthead

%     \caption[]{Tableau des Incertitudes (suite)} \\
%     \toprule
%     \textbf{Métrique} & \textbf{Outil} & \textbf{Précision} & \textbf{Sources et Biais} \\
%     \midrule
%     \endhead

%     \midrule
%     \multicolumn{4}{r}{\textit{Suite page suivante...}} \\
%     \endfoot

%     \bottomrule
%     \endlastfoot


%     \textbf{Conso. Électrique (W)} & 
%     Prise connectée (Tapo) sur secteur. & 
%     $\pm$ 5\% à $\pm$ 10\% \newline (Pour $P > 10$W) & 
%     \textbf{Rafraîchissement lent} ($\sim$1Hz) : La prise peut manquer des pics brefs. \newline
%     \textbf{Mesure globale} : Inclut les pertes de l'alim (AC/DC) et le système complet. \\
%     \midrule
    
%     \textbf{Puissance GPU/SoC} & 
%     Sondes logicielles (nvidia-smi, tegrastats). & 
%     $\pm$ 5\% \newline (Doc NVIDIA) & 
%     \textbf{Lissage temporel} : Moyenne sur un intervalle (ex: 200ms), masquant les micro-pics. \newline
%     \textbf{Pertes non comptées} : Ignore parfois la carte porteuse (USB, Ethernet) sur Jetson. \\
%     \midrule
    
%     \textbf{Latence (Temps)} & 
%     Chronométrage Python (perf\_counter). & 
%     $< 0.1$ ms \newline (Négligeable) & 
%     \textbf{Jitter (OS)} : Incertitude due à l'ordonnanceur de l'OS (interruptions). \newline
%     \textbf{Overhead} : Coût du code de mesure négligeable face à une inférence de 50ms. \\
%     \midrule
    
%     \textbf{Mémoire (RAM / VRAM)} & 
%     RSS (Resident Set Size) \& Système. & 
%     Exact \newline (à l'octet près) & 
%     \textbf{Biais d'interprétation} : Mesure la mémoire "réservée" (allouée), pas forcément celle activement utilisée (caching des frameworks). \\
%     \midrule
    
%     \textbf{Qualité (mAP)} & 
%     Dataset COCO128 \newline (Sous-ensemble). & 
%     Biais d'échantillonnage & 
%     \textbf{Taille du jeu de test} : 128 images est un échantillon faible. La mAP réelle sur COCO val2017 pourrait varier de $\pm$ 1-2\%. Ce jeu favorise la rapidité. \\

\end{xltabular}










% Pour benchmarker sur un hard donnée (même procédure pour tous les hards), on applique trois mesures to ensure fairness 1) mesure des métriques hardwares à vide pendant quelques secondes, sans faire tourner d'inférence, pour mesurer la consommation "de base" de l'OS et app en tâches de fond, 2) avant de lancer la vraie inférence, on fait une phase de préchauffage où vont va faire quelques run pour du beurre pour stabiliser le pipeline/modèle (expliquer un peu plus en détail ça), 3) on va répéter chaque expérience trois fois, et on prendra la moyenne des résultats.

% Ensuite pour chaque hardware, on lance le test du modèle sur le hadware
% On bench avec coco128, petit, rapide, mais une centaine d'image quand même avoir des résultats exploitables et on fait les mesures en logguant les résultats dans un CSV. On a utilisé une prise Tapo pour la conso électrique, on en parlera avant, mais là juste dire que la prise on la branche entre la source de courrant et cable d'alim du hardware, enfin on branche l'allim du hard sur la prise tapo sur le courrant.

% Parles des risques et incertitudes de mesures

\section{Résultats} \label{sec:resultats}

Nous analysons maintenant les résultats obtenus. Nous distinguons deux cas d’usage : (i) un scénario confortable (4070/i9) où l’objectif est de maximiser la qualité (mAP@50) tout en minimisant la latence (E2E p95), et (ii) un scénario plus \textit{edge} (Pi/Oak) où l’on cherche à minimiser la latence tout en minimisant la puissance électrique moyenne.

Pour la 4070 et le i9, nous représentons le compromis mAP@50 (à maximiser) vs E2E p95 (à minimiser) via un front de Pareto. Un point appartient au front s’il n’est dominé par aucun autre : autrement dit, il n’existe pas de configuration simultanément plus précise (mAP@50 plus élevé) et plus rapide (E2E p95 plus faible). Les \cref{fig:pareto_pc} ne montrent donc que les configurations meilleures candidates pour ce cas d’usage (12 sur 90 pour le 4070, et 16 sur 90 pour le i9).

\begin{figure}[H]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_4070.png}
  \caption*{(a) 4070}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_cpu.png}
  \caption*{(b) i9}
\end{minipage}
\caption{Fronts de Pareto pour représentant la mAP@50 à maximiser, et l'E2E p95 à minimiser. Les numéros renvoient aux configurations listées dans les \cref{tab:pareto_4070,tab:pareto_cpu}.}
\label{fig:pareto_pc}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} & \textbf{VRAM} \\
\hline
1  & s 256 fp32 ORT ALL      & 0.5942 & 4.6367  & 36.12 & 117.67 & 1866.75 & 11062.20 & 358.33 \\
2  & s 256 fp16 ORT ALL      & 0.5953 & 5.2967  & 18.09 & 102.33 & 1866.75 & 10991.55 & 283.00 \\
3  & m 256 fp16 ORT ALL      & 0.6770 & 5.6300  & 38.42 & 114.00 & 1856.30 & 10683.69 & 356.33 \\
4  & m 256 fp32 ORT DISABLE  & 0.6785 & 5.8367  & 76.75 & 150.33 & 1845.47 & 10740.47 & 483.00 \\
5  & m 256 fp16 ORT BASIC    & 0.6793 & 5.8633  & 38.42 & 118.33 & 1856.31 & 10691.94 & 357.67 \\
6  & m 320 fp32 ORT ALL      & 0.7034 & 6.8000  & 76.77 & 109.33 & 1871.49 & 10751.31 & 484.33 \\
7  & m 416 fp32 ORT ALL      & 0.7452 & 8.9733  & 76.79 & 134.67 & 1843.27 & 10674.11 & 486.33 \\
8  & m 416 fp16 ORT ALL      & 0.7487 & 10.0033 & 38.44 & 122.67 & 1853.58 & 10727.44 & 356.33 \\
9  & m 512 fp32 ORT ALL      & 0.7783 & 11.5300 & 76.83 & 139.00 & 1838.39 & 10605.55 & 741.67 \\
10 & m 512 fp16 ORT ALL      & 0.7788 & 12.6167 & 38.46 & 127.33 & 1842.44 & 10652.76 & 483.67 \\
11 & m 640 fp32 ORT ALL      & 0.7800 & 16.1633 & 76.89 & 121.33 & 1481.47 & 10292.72 & 734.33 \\
12 & m 640 fp16 ORT ALL      & 0.7809 & 17.0867 & 38.48 & 121.00 & 1813.53 & 10529.27 & 483.00 \\
\hline
\end{tabular}
\caption{Configurations non dominées (front de Pareto) sur RTX 4070 Laptop pour le cas d’usage \#1. RSS, RAM sys et VRAM sont en MB ; la puissance est en W.}
\label{tab:pareto_4070}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} \\
\hline
1  & n 256 fp32 ORT ALL   & 0.4589 & 12.2367  & 10.07 & 97.33  & 866.00  & 12459.79 \\
2  & n 256 fp16 ORT ALL   & 0.4590 & 13.1567  & 5.07  & 96.67  & 866.00  & 12397.68 \\
3  & n 320 fp16 ORT ALL   & 0.5154 & 15.8967  & 5.08  & 97.67  & 866.26  & 12490.68 \\
4  & s 256 fp32 ORT ALL   & 0.5943 & 17.1333  & 36.12 & 98.33  & 884.57  & 11820.61 \\
5  & s 320 fp32 ORT ALL   & 0.6486 & 21.5167  & 36.13 & 99.00  & 888.61  & 11749.86 \\
6  & s 320 fp16 ORT ALL   & 0.6499 & 24.0067  & 18.10 & 98.00  & 891.05  & 11827.09 \\
7  & m 256 fp32 ORT ALL   & 0.6783 & 29.3333  & 76.75 & 98.33  & 981.24  & 11451.16 \\
8  & s 416 fp32 ORT ALL   & 0.6911 & 33.3133  & 36.16 & 99.33  & 925.98  & 11672.47 \\
9  & m 320 fp32 ORT ALL   & 0.7034 & 39.6133  & 76.77 & 98.56  & 996.60  & 11177.24 \\
10 & s 512 fp32 ORT ALL   & 0.7307 & 42.3200  & 36.19 & 99.00  & 937.88  & 11445.71 \\
11 & s 512 fp16 ORT ALL   & 0.7309 & 47.3567  & 18.13 & 98.22  & 944.51  & 11634.59 \\
12 & m 416 fp32 ORT ALL   & 0.7452 & 61.3667  & 76.79 & 98.83  & 1061.00 & 11123.77 \\
13 & m 512 fp32 ORT ALL   & 0.7782 & 84.3167  & 76.83 & 99.17  & 1091.08 & 11063.42 \\
14 & m 640 fp32 ORT ALL   & 0.7802 & 122.7000 & 76.89 & 125.23 & 1210.44 & 11238.51 \\
15 & m 640 fp16 ORT ALL   & 0.7805 & 132.0133 & 38.48 & 99.67  & 1267.36 & 10793.08 \\
16 & m 640 fp16 ORT BASIC & 0.7806 & 207.3733 & 38.48 & 99.03  & 1207.62 & 10769.02 \\
\hline
\end{tabular}
\caption{Configurations non dominées (front de Pareto) sur CPU i9 pour le cas d’usage \#1. La VRAM n’est pas applicable en exécution CPU.}
\label{tab:pareto_cpu}
\end{table}

Sur GPU, un fait marquant est la forte représentation de la variante \texttt{m} (10/12) et du mode \texttt{ORT ALL} (10/12), suggérant que, lorsque le budget de calcul est suffisant, les optimisations de graphe ORT au niveau maximal contribuent fortement à obtenir des points à la fois rapides et précis. Sur CPU, le front couvre davantage de tailles (\texttt{n/s/m}) — ce qui reflète un compromis plus sensible entre complexité et latence — mais \texttt{ORT ALL} domine également (15/16), avec un unique point \texttt{ORT BASIC} nettement plus lent. Les tableaux permettent aussi de constater que (i) \texttt{FP16} réduit fortement la taille du modèle (et parfois la mémoire), mais (ii) n’apporte pas systématiquement un gain de latence sur ces configurations, ce qui rejoint l’observation globale discutée plus loin.

Dans un scénario plus \textit{edge}, où l’on suppose des ressources limitées, nous considérons le compromis puissance électrique moyenne (à minimiser) vs E2E p95 (à minimiser). Les fronts obtenus sont beaucoup plus restreints : 6 configurations pour la Pi et un unique point pour l’Oak dans cet extrait, ce qui reflète des contraintes fortes (notamment la nécessité de rester sur des modèles très légers et des résolutions faibles). Dans les deux cas, le front est constitué uniquement de variantes \texttt{n} ; sur Pi on observe essentiellement \texttt{ORT ALL} (et quelques points \texttt{ORT BASIC}), et sur OAK le point retenu correspond à \texttt{S6} (6 SHAVEs).

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_pi.png}
  \caption*{(a) Raspberry Pi 4}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_oak.png}
  \caption*{(b) OAK-D Pro (MyriadX)}
\end{minipage}
\caption{Fronts de Pareto pour le cas d’usage \#2 (puissance vs latence). Les numéros renvoient aux \cref{tab:pareto_pi,tab:pareto_oak}.}
\label{fig:pareto_edge}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} \\
\hline
1 & n 256 fp32 ORT ALL   & 0.4589 & 119.9933 & 10.07 & 5.8167 & 375.43 & 559.2933 \\
2 & n 256 fp16 ORT ALL   & 0.4590 & 122.5867 & 5.07  & 5.7500 & 375.43 & 556.0800 \\
3 & n 256 fp16 ORT BASIC & 0.4590 & 139.3500 & 5.07  & 5.7433 & 375.43 & 556.1000 \\
4 & n 416 fp16 ORT ALL   & 0.5801 & 288.0167 & 5.09  & 5.5900 & 375.42 & 555.6167 \\
5 & n 512 fp16 ORT ALL   & 0.6376 & 425.4933 & 5.11  & 5.1700 & 396.79 & 577.1433 \\
6 & n 512 fp16 ORT BASIC & 0.6376 & 440.4000 & 5.11  & 5.1633 & 404.66 & 585.7467 \\
\hline
\end{tabular}
\caption{Configurations non dominées (front de Pareto) sur Raspberry Pi 4 pour le cas d’usage \#2.}
\label{tab:pareto_pi}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} \\
\hline
1 & n 256 fp16 OAK S6 & 0.4567 & 72.6167 & 5.54 & 5.8667 & 386.2767 & 559.6400 \\
\hline
\end{tabular}
\caption{Configuration non dominée sur OAK-D Pro pour le cas d’usage \#2.}
\label{tab:pareto_oak}
\end{table}

Pour isoler l’effet des principaux leviers, nous analysons ensuite les métriques moyennes regroupées par facteur. Les \cref{fig:map_factors} confirment que la qualité (mAP@50) est principalement corrélée (i) à la taille du modèle (\texttt{m} $>$ \texttt{s} $>$ \texttt{n}) et (ii) à la résolution d’entrée (plus l’image est grande, plus la mAP@50 augmente), avec des tendances globalement cohérentes d’un matériel à l’autre. À l’inverse, dans nos mesures, la quantification FP16/FP32, le nombre de SHAVEs (OAK) et le niveau de fusion ORT n’ont pas montré d’impact significatif sur la mAP@50 (à l’échelle des variations observées).

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{map_per_scale.png}
  \caption*{(a) mAP@50 vs taille \texttt{n/s/m}}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{map_per_size.png}
  \caption*{(b) mAP@50 vs résolution}
\end{minipage}
\caption{Effet des principaux leviers de compilation sur la qualité (mAP@50).}
\label{fig:map_factors}
\end{figure}

Du côté des performances temporelles, les \cref{fig:e2e_factors} montrent une corrélation forte entre résolution et latence E2E : plus l’entrée est grande, plus l’inférence est lente, ce qui est attendu (plus de pixels à traiter). On notera que les barres Pi et OAK peuvent sembler proches à certains endroits car les valeurs sont agrégées : or la Pi n’exécute ici que des variantes \texttt{n}, tandis que l’OAK inclut aussi des variantes \texttt{s/m}, ce qui peut biaiser la comparaison directe si l’on ne contrôle pas la taille de modèle. Enfin, concernant les optimisations ORT, nous observons une légère dominance du mode \texttt{ALL} (et parfois \texttt{DISABLE}) par rapport à \texttt{BASIC}, ce dernier apparaissant en moyenne comme le plus lent sur ces regroupements.

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{e2e_size.png}
  \caption*{(a) E2E p95 vs résolution}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{e2e_fusion.png}
  \caption*{(b) E2E p95 vs fusion ORT}
\end{minipage}
\caption{Effet des facteurs principaux sur la latence E2E (p95).}
\label{fig:e2e_factors}
\end{figure}

Enfin, plusieurs tendances secondaires ressortent de l’ensemble des mesures : (i) FP16 apparaît très légèrement plus lent que FP32 sur certaines plateformes (écart faible), ce qui suggère que le gain attendu en calcul n’est pas toujours dominant face aux surcoûts de conversion, d’implémentation ou de pipeline selon le \textit{runtime} ; (ii) sur OAK, un nombre de SHAVEs plus faible tend à aller de pair avec une latence légèrement plus élevée ; (iii) la résolution d’entrée a un impact modéré sur la puissance moyenne (plus grand $\Rightarrow$ légèrement plus gourmand), et FP32 tend à consommer un peu plus que FP16 ; (iv) à l’inverse, les niveaux de fusion ORT et le nombre de SHAVEs montrent peu de variations de puissance dans nos relevés.

% % Faire le pont avec section précédentes, on va maintenant analyse les résultats.
% % En premier, on s'intéresse aux GPU du PC et au CPU. On considère un cas d'usage pas trop edge, ou on a du budget et de la puissance, et on s'intéresse plutôt à  maxer le map50 moyen, et minimiser l'E2E.

% Pour CPU et GPU c'est représenté avec un graph de pareto (expliquer brièvement ce que ça veut dire, le fait qu'on représente que les configurations qui domine les autres), on trouve 12 configurations alternatives (sur 90 du coup) pour le 4070, et 9 (idem) pour le CPU. fait intéressant, pour le GPU, on a 10 modèles sur 12 qui sont m, et 10 sur 12 qui sont ORT ALL.

% Pour le CPU on a un peu de tout niveau nsm, par contre 15 sur 16 sont ORT ALL. Donc pour ce cas d'usage, on semble comprendre que ORT ALL compte bcp pour trouver des modèles rapides et précis quand on a bcp de budgets.

% La, mettre les deux images cotes a cotes

% Et en dessous le tableau avec les labels et modèles correspondant

% 	Pareto_id	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	s 256 fp32 ORT ALL	0.5942	4.636667	36.12	117.666667	1866.750000	11062.200000	358.333333
% 1	2	s 256 fp16 ORT ALL	0.5953	5.296667	18.09	102.333333	1866.750000	10991.546667	283.000000
% 2	3	m 256 fp16 ORT ALL	0.6770	5.630000	38.42	114.000000	1856.300000	10683.690000	356.333333
% 3	4	m 256 fp32 ORT DISABLE	0.6785	5.836667	76.75	150.333333	1845.473333	10740.473333	483.000000
% 4	5	m 256 fp16 ORT BASIC	0.6793	5.863333	38.42	118.333333	1856.310000	10691.936667	357.666667
% 5	6	m 320 fp32 ORT ALL	0.7034	6.800000	76.77	109.333333	1871.490000	10751.306667	484.333333
% 6	7	m 416 fp32 ORT ALL	0.7452	8.973333	76.79	134.666667	1843.270000	10674.106667	486.333333
% 7	8	m 416 fp16 ORT ALL	0.7487	10.003333	38.44	122.666667	1853.580000	10727.436667	356.333333
% 8	9	m 512 fp32 ORT ALL	0.7783	11.530000	76.83	139.000000	1838.390000	10605.546667	741.666667
% 9	10	m 512 fp16 ORT ALL	0.7788	12.616667	38.46	127.333333	1842.440000	10652.756667	483.666667
% 10	11	m 640 fp32 ORT ALL	0.7800	16.163333	76.89	121.333333	1481.473333	10292.720000	734.333333
% 11	12	m 640 fp16 ORT ALL	0.7809	17.086667	38.48	121.000000	1813.526667	10529.270000	483.000000

% parler vite fait de trucs intéressants dans le tableau

% 	Pareto_id	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	n 256 fp32 ORT ALL	0.4589	12.236667	10.07	97.333333	866.000000	12459.790000	NaN
% 1	2	n 256 fp16 ORT ALL	0.4590	13.156667	5.07	96.666667	866.000000	12397.683333	NaN
% 2	3	n 320 fp16 ORT ALL	0.5154	15.896667	5.08	97.666667	866.260000	12490.683333	NaN
% 3	4	s 256 fp32 ORT ALL	0.5943	17.133333	36.12	98.333333	884.566667	11820.610000	NaN
% 4	5	s 320 fp32 ORT ALL	0.6486	21.516667	36.13	99.000000	888.610000	11749.860000	NaN
% 5	6	s 320 fp16 ORT ALL	0.6499	24.006667	18.10	98.000000	891.053333	11827.093333	NaN
% 6	7	m 256 fp32 ORT ALL	0.6783	29.333333	76.75	98.333333	981.243333	11451.163333	NaN
% 7	8	s 416 fp32 ORT ALL	0.6911	33.313333	36.16	99.333333	925.980000	11672.466667	NaN
% 8	9	m 320 fp32 ORT ALL	0.7034	39.613333	76.77	98.556667	996.603333	11177.236667	NaN
% 9	10	s 512 fp32 ORT ALL	0.7307	42.320000	36.19	99.003333	937.880000	11445.713333	NaN
% 10	11	s 512 fp16 ORT ALL	0.7309	47.356667	18.13	98.220000	944.510000	11634.593333	NaN
% 11	12	m 416 fp32 ORT ALL	0.7452	61.366667	76.79	98.833333	1061.000000	11123.773333	NaN
% 12	13	m 512 fp32 ORT ALL	0.7782	84.316667	76.83	99.166667	1091.083333	11063.420000	NaN
% 13	14	m 640 fp32 ORT ALL	0.7802	122.700000	76.89	125.233333	1210.436667	11238.510000	NaN
% 14	15	m 640 fp16 ORT ALL	0.7805	132.013333	38.48	99.666667	1267.356667	10793.080000	NaN
% 15	16	m 640 fp16 ORT BASIC	0.7806	207.373333	38.48	99.026667	1207.616667	10769.016667	NaN



% Ensuite deuxième cas d'usage, plus edge, on a un des ressources limitées donc on veut minimiser l'E2E tout en minimisant la conso élec, par exemple, là on voit qu'on a que 6 modèles pour Pi, et 1 seul pour Oak. dans les deux cas, c'est que des modèles n, avec ort basic ou all sur pi, et 6 shaves sur la oak.

% mettre les deux graphes pareto

% et en dessous tableau ...

% Pareto_id_power	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	n 256 fp32 ORT ALL	0.4589	119.993333	10.07	5.816667	375.43	559.293333	NaN
% 1	2	n 256 fp16 ORT ALL	0.4590	122.586667	5.07	5.750000	375.43	556.080000	NaN
% 2	3	n 256 fp16 ORT BASIC	0.4590	139.350000	5.07	5.743333	375.43	556.100000	NaN
% 3	4	n 416 fp16 ORT ALL	0.5801	288.016667	5.09	5.590000	375.42	555.616667	NaN
% 4	5	n 512 fp16 ORT ALL	0.6376	425.493333	5.11	5.170000	396.79	577.143333	NaN
% 5	6	n 512 fp16 ORT BASIC	0.6376	440.400000	5.11	5.163333	404.66	585.746667	NaN


% 	Pareto_id_power	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	n 256 fp16 OAK S6	0.4567	72.616667	5.54	5.866667	386.276667	559.64	NaN

% parler aussi vite fait des tableau

% Ensuite, pour l'analyse de l'impact des performances des optimisations, quand on étudie le bargraphe du map50 moyen par quantification, on constate que la map est systématiquement corrélée avec la taille du modèle, plus il est gros, plus c'est élevé, et d'ailleurs ça varie pas bcp d'un hard à l'autre par contre.

% pour la map moy par input size, mêmes observations, plus elle est élevée, plus la map augmente.

% la mettre les deux graphes map/quantif et map/input size cote cote

% pour la quantification, le nb de shaves et la fusion ort, on a observée aucun impact significatif sur le map.

% pour la E2E par input size pareil c'est fortement corrélé, plus c'est grand, plus c'est lent. on précise sur le graphe que oak et pi semblent aux mêmes niveaux parce qu'on a moyenné les vvaleurs mais que pi a eu que du nano alors que oak a eu du m et s

% pour le E2E par fusion, on observe une légère dominance du mode all, et disable, et etonnament basic le plus lent.

% la mettre les deux graphes cote cote

% pour fp16/32, expliquer qu'on a observé (pas de graphe ici) que fp16 est très légèrement plus lent que 32.

% que pour les shaves, moins de shaves semble aller de pair avec légèrement plus lent

% que l'input size à un impact très modéré sur la conso en watt, avec une plus grande taille indiquant généralement une plus grosse conso

% idem pour fp16/32, avec fp32 plus gourmand

% pas de différences entre les shaves sur la conso et peu de variation entre les niveaux de fusions et la conso.

\section{Analyse et conclusion} \label{sec:analyse_et_conclusion}

\section*{Annexe}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|c|c|c|}
\hline
\textbf{Metric} & \textbf{Hardware} & \textbf{n} & \textbf{s} & \textbf{m} \\
\hline
\multirow{4}{*}{E2E p95 (ms)} 
& 4070 & 9.05 & \textbf{9.0} & 10.64 \\
& Oak & \textbf{181.82} & 268.12 & 526.87 \\
& Orin & \textbf{35.9} & 43.06 & 62.78 \\
& i9 & \textbf{27.15} & 46.11 & 95.75 \\
\hline
\multirow{4}{*}{mAP@50 (\%)} 
& 4070 & 57.23 & 68.02 & \textbf{73.7} \\
& Oak & 57.1 & 67.86 & \textbf{73.67} \\
& Orin & 57.21 & 68.0 & \textbf{73.72} \\
& i9 & 57.24 & 68.03 & \textbf{73.65} \\
\hline
\multirow{1}{*}{CPU util (\%)} 
& i9 & 73.02 & 71.38 & \textbf{70.1} \\
\hline
\multirow{2}{*}{GPU util (\%)} 
& 4070 & \textbf{13.79} & 16.53 & 24.67 \\
& Orin & \textbf{32.23} & 45.24 & 60.69 \\
\hline
\multirow{2}{*}{RAM (MB)} 
& 4070 & 1053.94 & 1053.69 & \textbf{1006.45} \\
& i9 & \textbf{106.03} & 153.3 & 265.17 \\
\hline
\multirow{1}{*}{VRAM (MB)} 
& 4070 & \textbf{-57.15} & 12.73 & 157.13 \\
\hline
\multirow{3}{*}{Power mean (W)} 
& 4070 & \textbf{38.76} & 45.18 & 55.64 \\
& Oak & \textbf{1.45} & 1.6 & 1.86 \\
& Orin & \textbf{1.25} & 1.82 & 2.5 \\
\hline
\end{tabular}
\caption{Mesures en fonction des niveaux d'élagage}
\label{tab:pruning}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|c|c|c|}
\hline
\textbf{Metric} & \textbf{Hardware} & \textbf{ALL} & \textbf{BASIC} & \textbf{DISABLE} \\
\hline
\multirow{3}{*}{E2E p95 (ms)} 
& 4070 & \textbf{9.36} & 9.62 & 9.71 \\
& Pi & \textbf{321.89} & 346.64 & 336.4 \\
& i9 & \textbf{43.43} & 62.61 & 62.98 \\
\hline
\multirow{3}{*}{mAP@50 (\%)} 
& 4070 & \textbf{66.32} & 66.31 & 66.31 \\
& Pi & \textbf{57.24} & \textbf{57.24} & \textbf{57.24} \\
& i9 & \textbf{66.31} & \textbf{66.31} & \textbf{66.31} \\
\hline
\multirow{2}{*}{CPU util (\%)} 
& Pi & 97.56 & 97.04 & \textbf{96.91} \\
& i9 & \textbf{70.15} & 73.1 & 71.25 \\
\hline
\multirow{1}{*}{GPU util (\%)} 
& 4070 & \textbf{15.15} & 19.15 & 20.69 \\
\hline
\multirow{3}{*}{RAM (MB)} 
& 4070 & \textbf{1037.52} & 1038.63 & 1037.93 \\
& Pi & 116.66 & 119.08 & \textbf{110.9} \\
& i9 & 187.05 & \textbf{168.63} & 168.82 \\
\hline
\multirow{1}{*}{VRAM (MB)} 
& 4070 & \textbf{35.87} & 37.4 & 39.44 \\
\hline
\multirow{3}{*}{Power mean (W)} 
& 4070 & \textbf{43.21} & 47.68 & 48.69 \\
& Pi & \textbf{2.66} & 2.72 & 2.72 \\
& i9 & 64.71 & \textbf{63.53} & \textbf{63.53} \\
\hline
\end{tabular}
\caption{Mesures en fonction des niveaux de fusion}
\label{tab:fusion}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{H0} & \textbf{H1} \\
\hline
\multirow{1}{*}{E2E p95 (ms)} 
& \textbf{47.08} & 47.42 \\
\hline
\multirow{1}{*}{mAP@50 (\%)} 
& \textbf{66.32} & 66.3 \\
\hline
\multirow{1}{*}{GPU util (\%)} 
& \textbf{45.88} & 46.23 \\
\hline
\multirow{1}{*}{Power mean (W)} 
& 1.87 & \textbf{1.84} \\
\hline
\end{tabular}
\caption{Mesures en fonction des niveaux d'\textit{heuristic}}
\label{tab:heuristic}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Metric} & \textbf{Hardware} & \textbf{fp16} & \textbf{fp32} \\
\hline
\multirow{4}{*}{E2E p95 (ms)} 
& 4070 & 10.08 & \textbf{9.05} \\
& Orin & \textbf{42.51} & 51.99 \\
& Pi & 346.74 & \textbf{323.2} \\
& i9 & 58.44 & \textbf{54.24} \\
\hline
\multirow{4}{*}{mAP@50 (\%)} 
& 4070 & \textbf{66.34} & 66.29 \\
& Orin & \textbf{66.32} & 66.3 \\
& Pi & \textbf{57.28} & 57.21 \\
& i9 & \textbf{66.32} & 66.29 \\
\hline
\multirow{2}{*}{CPU util (\%)} 
& Pi & \textbf{95.4} & 98.94 \\
& i9 & \textbf{68.46} & 74.54 \\
\hline
\multirow{2}{*}{GPU util (\%)} 
& 4070 & \textbf{13.1} & 23.56 \\
& Orin & \textbf{34.77} & 57.33 \\
\hline
\multirow{3}{*}{RAM (MB)} 
& 4070 & 1048.76 & \textbf{1027.3} \\
& Pi & \textbf{114.72} & 116.38 \\
& i9 & 191.7 & \textbf{157.97} \\
\hline
\multirow{1}{*}{VRAM (MB)} 
& 4070 & \textbf{-7.57} & 82.71 \\
\hline
\multirow{4}{*}{Power mean (W)} 
& 4070 & \textbf{38.55} & 54.5 \\
& Orin & \textbf{1.49} & 2.22 \\
& Pi & \textbf{2.55} & 2.85 \\
& i9 & \textbf{63.56} & 64.29 \\
\hline
\end{tabular}
\caption{Mesures en fonction des niveaux de quantisation}
\label{tab:quantization}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{Hardware} & \textbf{256} & \textbf{320} & \textbf{416} & \textbf{512} & \textbf{640} \\
\hline
\multirow{5}{*}{E2E p95 (ms)} 
& 4070 & \textbf{5.4} & 7.08 & 8.85 & 11.15 & 15.34 \\
& Oak & \textbf{105.71} & 188.24 & 317.69 & 379.91 & 636.45 \\
& Orin & \textbf{24.12} & 30.29 & 43.63 & 59.13 & 79.08 \\
& Pi & \textbf{129.68} & 188.09 & 292.61 & 426.58 & 637.9 \\
& i9 & \textbf{24.94} & 34.86 & 50.85 & 68.54 & 102.5 \\
\hline
\multirow{5}{*}{mAP@50 (\%)} 
& 4070 & 57.73 & 62.26 & 67.14 & 71.55 & \textbf{72.91} \\
& Oak & 57.51 & 62.09 & 67.42 & 70.92 & \textbf{73.1} \\
& Orin & 57.7 & 62.24 & 67.18 & 71.55 & \textbf{72.89} \\
& Pi & 45.9 & 51.49 & 57.88 & 63.77 & \textbf{67.18} \\
& i9 & 57.71 & 62.24 & 67.09 & 71.55 & \textbf{72.94} \\
\hline
\multirow{2}{*}{CPU util (\%)} 
& Pi & 100.69 & 98.61 & 96.94 & \textbf{94.36} & 95.26 \\
& i9 & 77.42 & 69.67 & \textbf{67.81} & 73.31 & 69.29 \\
\hline
\multirow{2}{*}{GPU util (\%)} 
& 4070 & 16.16 & 16.65 & \textbf{14.37} & 17.31 & 27.15 \\
& Orin & 45.98 & 44.28 & 45.18 & \textbf{44.53} & 50.3 \\
\hline
\multirow{3}{*}{RAM (MB)} 
& 4070 & 1051.09 & 1057.88 & 1051.42 & 1047.68 & \textbf{982.06} \\
& Pi & \textbf{82.64} & \textbf{82.64} & 89.71 & 106.17 & 216.58 \\
& i9 & \textbf{122.49} & 132.42 & 162.55 & 192.98 & 263.74 \\
\hline
\multirow{1}{*}{VRAM (MB)} 
& 4070 & \textbf{-9.11} & 0.52 & 3.7 & 78.38 & 114.37 \\
\hline
\multirow{5}{*}{Power mean (W)} 
& 4070 & \textbf{43.46} & \textbf{43.46} & 46.04 & 48.67 & 51.0 \\
& Oak & \textbf{1.27} & 1.46 & 1.7 & 1.87 & 1.88 \\
& Orin & \textbf{1.44} & 1.63 & 1.87 & 2.05 & 2.28 \\
& Pi & 2.79 & 2.84 & 2.79 & 2.54 & \textbf{2.53} \\
& i9 & \textbf{63.32} & 63.53 & 63.62 & 63.81 & 65.34 \\
\hline
\end{tabular}
\caption{Mesures en fonction des niveaux de résolution}
\label{tab:resolution}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\hline
\multirow{1}{*}{E2E p95 (ms)} 
& 337.97 & 338.85 & 319.66 & 317.47 & \textbf{314.05} \\
\hline
\multirow{1}{*}{mAP@50 (\%)} 
& \textbf{66.21} & \textbf{66.21} & \textbf{66.21} & \textbf{66.21} & \textbf{66.21} \\
\hline
\multirow{1}{*}{Power mean (W)} 
& 1.69 & \textbf{1.59} & 1.66 & 1.63 & 1.6 \\
\hline
\end{tabular}
\caption{Mesures en fonction du nombre de SHAVEs}
\label{tab:threads}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{SP0} & \textbf{SP1} \\
\hline
\multirow{1}{*}{E2E p95 (ms)} 
& 47.31 & \textbf{47.19} \\
\hline
\multirow{1}{*}{mAP@50 (\%)} 
& \textbf{66.31} & 66.3 \\
\hline
\multirow{1}{*}{GPU util (\%)} 
& 46.67 & \textbf{45.44} \\
\hline
\multirow{1}{*}{Power mean (W)} 
& 1.87 & \textbf{1.84} \\
\hline
\end{tabular}
\caption{Mesures en fonction du niveau de \textit{sparsity}}
\label{tab:sparse}
\end{table}


% \section{Description du système}
% \subsection{Identification des besoins capteurs} %Identification of the sensors, their sources of uncertainty, and working ranges
% \subsection{Caracteristiques des capteurs }
% \subsection{Sources d'incertitude}
% \section{Sensor data pre-processing stage}
% \subsection{Architecture du Pipeline d'Inférence sur OAK-D}
% \section{NN architecture and computation graph description (il faut parler de yolo ici?)}
% \subsection{YOLO11}
% \section{Model optimisation process}
% \subsection{Ingestion et Standardisation des Données}
% \subsection{Entraînement et Transfer Learning}
% \subsection{Génération Automatisée de Variantes}
% \subsection{Compilation Spécifique au Matériel}
% \section{Résultat} %Benchmark results
% \subsection{Criteres d'évaluation}
% \subsection{résultats par rapport au méteriel}
% \subsection{résultat par rapport au modèle }
% \section{Github repo and documentation}
\end{document}
