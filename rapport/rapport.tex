\documentclass[12pt,a4paper]{article}

% Encodage et polices
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% Packages essentiels
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{xurl}

% Mise en page
\usepackage{geometry}
\geometry{margin=25mm}

\usepackage{setspace}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% En-têtes et pieds de page
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Université Côte d'Azur}
\fancyhead[R]{\leftmark}
\fancyfoot[L]{Noé Florence Report}
\fancyfoot[C]{Page \thepage/23}
\fancyfoot[R]{\today}

\fancypagestyle{plain}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% Hyperliens (à charger en dernier)
\usepackage{hyperref}
\usepackage[french]{cleveref}

\begin{document}
% Titre et auteur
\title{}
\author{}
\date{}

\begin{titlepage}
    \centering
    \includegraphics[width=0.6\textwidth]{UCA-Logo-2niveaux-RVB.png}

    {\scshape\LARGE University Côte d'Azur \par}
    \vspace{1.5cm}

    \textsc{\Large Edge Computing et IA embarquée }\par
    \vspace{1.5cm}
    
    {\Huge\bfseries Optimisation de la détection d'objets embarquée pour système IOT-CPS : Approche YOLOv11  \par}
    \vspace{2cm}
    {\Large\itshape 
    \textbf{Author:} \\  Noé Florence, Nathan Amoussou, Louis  \par

    }
    \vspace{1cm}

        {\large \textbf{Professeur:} \\  Gerad Rocher\par}
    \vfill

    {\large \textbf{ } \\
 \par}
    \vfill
    {\large \textbf{} \\ \par}
   \vfill
    
    % Facultatif : Superviseur
    \vspace{0.8cm}
    
\end{titlepage}



% Appliquer le style 'plain' à la première page pour qu'elle n'ait pas d'en-têtes ni de pieds de page
\thispagestyle{plain}

% --- Contenu du Document ---
\newpage




\maketitle

% \section{Context of the project and objectives}

\section{Introduction}

% Rappel du projet et de son intérêt, optimiser modèles IA parce que puissance de calcul limitée sur de l'embarquée et parfois pas de possibilitée pratique d'utiliser un modèle dans le cloud, et aussi parce que la pratique d'optimiser des modèles via différents levier, sur le hardware et directement le modèle, est répandue et accessible aujourd'hui

% Dans ce projet décrit dans ce rapport on va prendre un modèle d'ia et une sélection de hardware (décrit dans section ...), appliquer différentes couches et combinaisons d'optimisations (décrit dans scetion ...), et évaluer les performances et différentes métriques de consommation et performance sur différents hardwares en s'assurant que ces comparaisons sont le plus fair possible (décrit dans ...) et conclure sur la ou les combinaisons d'optim les plus intéressantes par hardware.

L’essor de l'informatique embarquée (ville intelligente, robotique, véhicules autonomes, santé connectée, etc.) s’accompagne d’un besoin croissant de déployer des modèles d’intelligence artificielle (IA) au plus proche des capteurs et des actionneurs. % Vérifier terme informatique embarquée
Ce déplacement du \textit{cloud} vers l’\textit{edge} est motivé à la fois par des contraintes pratiques (connectivité intermittente, coûts de transfert, confidentialité) et par des exigences techniques (faible latence, robustesse en temps réel), tout en restant limité par les ressources disponibles sur l’embarqué (mémoire, puissance de calcul, énergie). % Expliquer ou remplacer edge et cloud
Dans ce contexte, l’optimisation de modèles---via des leviers agissant sur le modèle lui-même et sur l’exploitation du matériel---est devenue une pratique répandue et de plus en plus accessible grâce aux outils et écosystèmes actuels.

Dans cette logique, le projet présenté dans ce rapport consiste à, partant d’un modèle d’IA et d’une sélection de plateformes matérielles (décrites en \cref{sec:contexte}), appliquer plusieurs couches d’optimisations (décrites en \cref{sec:benchmarking,subsec:optimisations_appliquees}). 
Nous mesurons ensuite l’impact de ces choix sur des métriques de performance et de coût en veillant à rendre les comparaisons aussi équitables que possible via un protocole commun (décrit en \cref{subsec:protocole_de_benchmarking,sec:resultats}). 
L’objectif final est d’identifier, pour chaque plateforme, les compromis les plus pertinents et de conclure sur la ou les combinaisons d’optimisations offrant le meilleur équilibre entre efficacité et qualité d’inférence, par exemple (\cref{sec:analyse_et_conclusion}). 

\section{Cas d'usage choisi} \label{sec:contexte} 
% TODO : Modifier le titre

\subsection{Architecture à optimiser} % TODO : Modifier le titre/utiliser un \paragraph?

% Comme modèle d'IA à optimiser, on a choisit un modèle de vision/détection, c'est un modèle d'IA, qui reçoit une image en entrée (ou une vidéo sous forme d'une succession continue d'image) et retourne la position d'un ou de plusieurs objets spécifiques détectées dans l'image, objets pour lesquels ce modèle à été entrainé spécifiquement. Le modèle spécifique c'est YOLOv11 d'ultralytic, un modèle publié en telle année, avec telles caractéristiques (taille, cas d'usage, recognition, etc.). On a choisit ce modèle parce que il est représentatif des modèles/cas d'usage utilisées en EDGE/Embarqué/IOT, et aussi parce que pour faire le pont avec nos nécessité pour le porjet d'IOT, on s'intéresse aux modèles de vision.

Comme architecture d'IA à optimiser, nous avons choisi un modèle de vision par ordinateur dédié à la détection d’objets. 
Ce type de réseau reçoit en entrée une image (ou une séquence d’images pour une vidéo) et produit en sortie un ensemble de prédictions décrivant où se trouvent les objets d’intérêt et de quel type ils sont : typiquement des boîtes englobantes (\textit{bounding boxes}), associées à une classe et à un score de confiance. 
Dans la suite, l’ensemble de nos optimisations viseront donc à accélérer et/ou alléger cette chaîne d’inférence tout en contrôlant la dégradation éventuelle de la qualité de détection.

Dans ce cadre, nous choisissons \href{https://docs.ultralytics.com/fr/models/yolo11/}{YOLO11}, une famille de détecteurs d’objets temps réel publiée par l’entreprise américaine Ultralytics en septembre 2024.
Cette gamme est conçue pour offrir un bon compromis entre précision, vitesse et efficacité et est explicitement pensée pour être déployée dans des environnements variés (du GPU aux dispositifs \textit{edge}).
Ce choix est motivé à la fois par le caractère très représentatif de YOLO pour des cas d’usage \textit{edge}/embarqué, et par l’adéquation avec nos besoins de projet centrés sur la vision.

\subsection{Matériel}

% Matériels sur lesquels optimiser puis tester le modèle d'IA divisé en deux caté :
% 1) Réference, non-edge, non embarqué, pas notre cible principal, à titre indicatif et de comparaison avec edge
% - un GPU 4070 laptop, 8GB vRAM, 32GB RAM
% - Un CPU i9 14900 HX 32GB ram
% 2) Matériel edge pour notre cas d'usage
% - une Pi4 (avec son CPU, checker en ligne)
% - un nvidia jetson orin nano 4GB (on s'intéresse au GPU)
% - une luxonis OAK D Pro (on s'intéresse au VPU myriad accélérateur)

À la suite du choix de l’architecture IA et afin d’évaluer l’effet des optimisations dans des conditions réalistes, nous testons l’inférence sur un ensemble de plateformes hétérogènes. 
Nous distinguons d’abord deux machines de référence---qui ne constituent pas notre cible principale \textit{edge}/embarqué, mais servent de point de comparaison indicatif---: un PC portable équipé d’un GPU NVIDIA RTX 4070 (8 Go de VRAM) et de 32 Go de RAM (nommé \textit{4070} dans la suite de ce document), ainsi qu’un CPU Intel Core i9-14900HX (24 cœurs / 32 \textit{threads}, 32 Go de RAM, nommé ensuite \textit{i9}).

Pour coller à notre cas d’usage \textit{edge}, nous retenons également trois dispositifs représentatifs de familles d’accélération différentes. 
Le Raspberry Pi 4 (nommé ensuite \textit{Pi}) constitue une cible CPU-\textit{only} (Cortex-A72, quatre cœurs/\textit{threads}) où l’on cherche surtout à maîtriser les performances sous fortes contraintes. 
Le NVIDIA Jetson Orin Nano 4GB (nommé ensuite \textit{Orin}) apporte un GPU embarqué (architecture Ampere, 512 cœurs CUDA et 16 Tensor Cores) et 4 Go de LPDDR5, ce qui en fait une plateforme adaptée à l’exploration d’optimisations tirant parti du calcul parallèle. 
Enfin, la Luxonis OAK-D Pro (nommé ensuite \textit{Oak}) embarque un VPU Movidius Myriad X, typique d’une approche utilisant un accélérateur dédié pour exécuter des modèles de vision efficacement à faible consommation.

\section{Optimisations} \label{sec:benchmarking}

% À partir du modèle et des hardwares clarifiés au-dessus, on a établi une liste de métriques de mesure d'une part dans subsec..., et d'optimisation dans subsec...

\subsection{Contexte} \label{subsec:optimisations_appliquees}

% TODO : Parler du fallback CPU sur INT8

% Décrire bien les pipelines

% Sur le modèle de vision retenue, on a appliqué ces optimisations (parfois séquentiellement parfois parallèlement);
% - élagage (n, s, m)
% - quantification (fp32, fp16)
% - input resolution de l'image dans le modèle
% On a aussi des optims spéxifiques à chaque hardware
% - les différents runtimes en fonctino du hard
% - fusion de couche en fonction du rt
% - nb de shaves sur oak
% - optims bins et sparsity sur trt et int8
%  Ce qui nous fait un total de X variantes par hard/rt

Afin de structurer notre exploration, nous regroupons les optimisations en quatre familles correspondant à un découpage $2\times2$ :
(i) optimisations à la compilation vs (ii) optimisations au \textit{runtime}, et, dans chaque cas,
(iii) optimisations agnostiques du matériel vs (iv) optimisations spécifiques à une plateforme.
Intuitivement, la compilation regroupe tout ce qui transforme le modèle avant l’exécution (export, conversion de format, génération d’un moteur, choix de précision, etc.), tandis que le \textit{runtime} correspond aux réglages appliqués au moment de l’inférence/l'utilisation (p.\,ex. fusions de couches activées par le moteur, choix du niveau d’optimisation du graphe).
Dans notre protocole, nous nous appuyons sur trois moteurs d’exécution (de \textit{runtime}) :
\begin{enumerate}
\item ONNX Runtime (ORT) pour exécuter des modèles au format ONNX (format standard décrivant un modèle IA sous forme d'un graphe de calcul),
\item TensorRT (TRT) sur matériel NVIDIA (génération d’un modèle IA sous un format \textit{engine} optimisé puis exécution),
\item et le \textit{runtime} DepthAI (DRT) sur Oak (exécution d’un modèle compilé au format \texttt{.blob} pour le VPU MyriadX).
\end{enumerate}
Le \cref{tab:runtimes_par_plateforme} récapitule, pour chaque plateforme matérielle, le ou les moteurs d’exécution (\emph{runtime}) retenus afin d’assurer une comparaison cohérente entre cibles.

\begin{table}[ht]
\centering
\begin{tabular}{l l}
\hline
\textbf{Matériel cible} & \textbf{Moteur(s) utilisé(s)} \\
\hline
i9 & ORT \\
4070 & ORT \\
Pi & ORT \\
Orin & ORT \& TRT \\
Oak & DRT \\
\hline
\end{tabular}
\caption{Correspondance entre plateformes matérielles et moteurs d’exécution considérés.}
\label{tab:runtimes_par_plateforme}
\end{table}

\newpage

\subsection{Optimisations à la compilation}

\subsubsection{Optimisations agnostiques matériel}

Notre base d’expériences repose sur trois leviers, indépendants de la plateforme, qui modifient directement la complexité et/ou le coût numérique du modèle :
\begin{itemize}
\item Taille du modèle (élagage/variante de YOLO) : \{\texttt{n}, \texttt{s}, \texttt{m}\}.
Une variante plus petite d'un modèle d'IA réduit typiquement le nombre de paramètres et d’opérations, donc la latence et la mémoire, au prix d’une précision potentiellement moindre.
\item Résolution d’entrée du modèle : \{640, 512, 416, 320, 256\}.
Une résolution d'image donnée en entrée au modèle plus faible diminue le coût de calcul (moins de pixels à traiter) et accélère l’inférence, mais peut dégrader la détection des petits objets.
\item Quantification du modèle (précision flottante) : \{\texttt{FP32}, \texttt{FP16}\}.
Passer en \texttt{FP16} réduit la bande passante mémoire et peut accélérer l’exécution sur du matériel qui l’exploite bien, avec un impact généralement limité sur la qualité.
\end{itemize}
% Ces trois axes définissent $3\times 5\times 2 = 30$ variantes de base.

\subsubsection{Optimisations spécifiques matériel}

Certaines optimisations dépendent directement de l’accélérateur ciblé :
\begin{itemize}
\item Oak --- nombre de SHAVEs : \{4, 5, 6, 7, 8\}.
Les SHAVEs sont des processeurs vectoriels internes ; en allouer davantage peut améliorer le débit de certaines opérations, au prix d’une contention possible avec d’autres traitements sur l’appareil.
\item Orin + TRT --- niveau d’optimisation du compilateur : \{3, 4, 5\}.
Un niveau plus élevé peut permettre au moteur de compilation de chercher des stratégies plus performantes (souvent au prix d’un temps de compilation plus long).
\item Orin + TRT --- \textit{sparsity} : \{\texttt{false}, \texttt{true}\}.
Lorsque le modèle et le matériel le permettent, activer la \emph{sparsity} vise à tirer parti d’une accélération matérielle pour des poids structurés clairsemés.
\end{itemize}

\subsection{Optimisations au \textit{runtime} (agnostiques matériel)}

\subsubsection{Optimisations agnostiques matériel}

Au moment de l’exécution, nous évaluons un réglage portable via ORT :
\begin{itemize}
  \item Fusions/optimisations de graphe ORT : \{\texttt{DISABLE\_ALL}, \texttt{ENABLE\_BASIC}, \texttt{ENABLE\_EXTENDED}, \texttt{ENABLE\_ALL}\}.
  L’objectif est de réduire le coût d’exécution (moins d’opérations, meilleure localité mémoire) sans changer la sémantique du modèle.
\end{itemize}

\subsubsection{Optimisations spécifiques plateforme}

Nous n’avons pas retenu d’optimisations \textit{runtime} supplémentaires spécifiques à une plateforme (au-delà du choix du moteur lui-même, ORT vs TRT).

\subsection{Récapitulatif}

Le \cref{tab:optimisations_synthese} récapitule l’ensemble des optimisations étudiées, en distinguant celles appliquées à la compilation et au \textit{runtime}, ainsi que leur caractère agnostique ou spécifique à une plateforme.

\begin{table}[ht]
\centering
\begin{tabular}{c l l}
\hline
 & \textbf{Compilation} & \textbf{\textit{Runtime}} \\
\hline
\textbf{Agnostique} &
\begin{tabular}[t]{@{}l@{}}
  Élagage (3 niveaux) \\
  Résolution (5 niveaux) \\
  Quantification (2 niveaux)
\end{tabular}
&
Fusion (ORT, 3 niveaux)
\\
\hline
\textbf{Spécifique} &
\begin{tabular}[t]{@{}l@{}}
  SHAVEs (Oak, 5 niveaux) \\
  Optimisation (Orin + TRT, 3 niveaux) \\
  \textit{Sparsity} (Orin + TRT, 2 niveaux)
\end{tabular}
&
--- \\
\hline
\end{tabular}
\caption{Synthèse des optimisations évaluées, par famille (compilation/\textit{runtime}) et par portée (agnostique/spécifique).}
\label{tab:optimisations_synthese}
\end{table}

Ces optimisations sont globalement combinables : on peut, pour une plateforme donnée, composer les variantes "agnostiques compilation" (taille/résolution/précision) avec les options "spécifiques compilation" quand elles existent (p. ex. SHAVEs sur Oak, options de compilation sur TRT). En revanche, les réglages \textit{runtime} dépendent du moteur d’exécution : la fusion ORT ne s’applique qu’avec ONNX Runtime, et les optimisations TensorRT (niveau d’optimisation, \textit{sparsity}) ne concernent que les modèles compilés en \textit{engine} TRT et exécutés via TRT. De plus, selon le matériel, le point de départ n’est pas identique (p. ex. Oak : FP16 uniquement).

À partir de cette grille d’optimisations et des contraintes propres à chaque plateforme, nous pouvons établir ci-dessous un récapitulatif du nombre de variantes effectivement évaluables par matériel.

\begin{itemize}
\item 4070 (ORT) : $3 \times 5 \times 2 \times 3 = 90$ variantes.
\item i9 (ORT) : $3 \times 5 \times 2 \times 3 = 90$ variantes.
\item Pi (ORT, \texttt{yolo11n} uniquement) : $1 \times 5 \times 2 \times 3 = 30$ variantes.
\item Orin (ORT et TRT) :
\begin{itemize}
\item TRT : $3 \times 5 \times 2 \times 3 \times 2 = 180$ variantes,
\item ORT : $3 \times 5 \times 2 \times 3 = 90$ variantes,
\item Total Orin : $180 + 90 = 270$ variantes.
\end{itemize}
\item Oak (\texttt{FP16} uniquement, pas d'ORT) : $3 \times 5 \times 1 \times 5 = 75$ variantes.
\end{itemize}

\section{Métriques mesurées} \label{sec:metriques_mesurees}

% TODO : Développer et mieux expliquer nos choix

Après avoir défini les différentes optimisations (agnostiques et spécifiques) ainsi que les variantes effectivement testables sur chaque plateforme, l’étape suivante consiste à mesurer systématiquement les performances des couples (matériel, combinaison d’optimisations). 
Pour garder une lecture claire, nous regroupons les mesures en deux familles : (i) des métriques décrivant la performance et la qualité du modèle IA (\cref{subsec:mesures_de_performances}), et (ii) des métriques décrivant l’utilisation des ressources matérielles pendant l’inférence (CPU, mémoire, accélérateurs, etc., \cref{subsec:mesures_de_consommation}).

\subsection{Mesures de performances du modèle d'IA} \label{subsec:mesures_de_performances}

Nous évaluons la performance du modèle à la fois en termes de qualité de détection et de vitesse d’inférence. Concrètement, nous mesurons :
\begin{itemize}
  % \item \textbf{Taille du modèle (MB)} : taille du fichier exporté (\texttt{.onnx}, \texttt{.engine}, \texttt{.blob}), indicateur simple du \textit{coût de stockage} et, indirectement, des contraintes de déploiement/chargement sur l’embarqué.:contentReference[oaicite:1]{index=1}
  \item Temps d’inférence et latence de bout en bout :
  \begin{itemize}
    \item E2E (\textit{End-to-End}) : temps total que le modèle prend pour traiter chaque image incluant pré-traitement $\rightarrow$ inférence $\rightarrow$ post-traitement. 
    C’est la métrique la plus comparable entre plateformes, car elle reflète le temps réellement perçu dans une application.
    \item Décomposition pré-traitement / inférence / post-traitement : permet d’identifier si le goulot d’étranglement vient du modèle lui-même (lors de l'inférence) ou d’étapes périphériques du pré ou post-traitement (redimensionnement, NMS, etc.). 
    Selon le \textit{backend}, cette décomposition provient soit d’une mesure interne (p.\,ex. \texttt{Ultralytics result.speed}), soit d’un chronométrage explicite autour des étapes.
  \end{itemize}
  \item Percentiles de latence (p50, p95, p99) : ces percentiles résument la variabilité (\textit{jitter}) et les cas défavorables, qui sont souvent critiques en temps réel.
  \item Débit (FPS) : nombre d’images traitées par seconde (\textit{throughput}), utile pour estimer la capacité à suivre un flux vidéo et comparer l’efficacité de différentes optimisations à charge donnée.
  \item Qualité de détection :
  \begin{itemize}
    \item mAP@50 : mesure globale de performance de détection, ici avec un seuil d’IoU fixé à 0.50.
    \item Precision / Recall / F1 : précision des détections, capacité à retrouver les objets, et compromis entre les deux (via F1), afin de quantifier l’impact d’optimisations potentiellement destructrices (résolution plus faible, quantification, etc.).
  \end{itemize}
\end{itemize}

\subsection{Mesures de consommation du matériel} \label{subsec:mesures_de_consommation}

En complément, nous instrumentons l’exécution pour observer la pression exercée sur le système et mieux interpréter les temps mesurés. Les mesures suivantes sont collectées (moyenne, p95 et maximum lorsque pertinent) :
\begin{itemize}
  \item CPU (\%) : charge du processus lors du benchmark.
  \item Mémoire RAM (MB) :
  \begin{itemize}
    \item RSS du processus de benchmark (empreinte mémoire du programme),
    \item RAM système utilisée (pression mémoire globale), utile sur les plateformes à mémoire partagée (ex. Jetson).
  \end{itemize}
  \item Accélérateurs NVIDIA (RTX 4070 / Jetson Orin) :
  \begin{itemize}
    \item Utilisation GPU (\%) et VRAM (MB) via \texttt{nvidia-smi} sur PC,
    \item GR3D\_FREQ (\%) et EMC\_FREQ (\%) via \texttt{tegrastats} sur Jetson, pour approcher la charge GPU et la pression mémoire/bande passante.
  \end{itemize}
  \item OAK-D (VPU Myriad) : récupération d’indicateurs internes côté caméra (charge CPU embarquée \textit{Leon CSS}, mémoire \textit{DDR/CMX}) et, lorsque disponible, un temps d’inférence VPU estimé à partir des traces (\textit{profiling}).
  \item Consommation électrique (W) et énergie par inférence (J) : lorsque la mesure est disponible (p.\,ex. via un wattmètre externe), nous reportons la puissance instantanée et l’énergie moyenne par inférence, afin d’évaluer l’efficacité énergétique (métrique clé en embarqué).
\end{itemize}

% On a des métriques sur les performances du modèles, et des métriques sur l'utilisation du hardware.
% Pour le modèle, on mesure :
% - mAP
% - Precision/recall/F1
% - E2E runtime
% - device time
% - preprocess
% - inference
% - postprocess
% - p50/p...
% - FPS
% On a aussi des métriques côtés hardwares :
% - Utilisation de la puce
% - Utilisation de la RAM
% - température
% - conso élec

\section{Protocole de benchmarking} \label{subsec:protocole_de_benchmarking}

Après avoir défini l’espace des optimisations et des métriques relevées, l’enjeu devient de mesurer leurs effets de façon comparable d’une plateforme à l’autre. Nous appliquons un protocole identique sur chaque matériel, fondé sur trois précautions : (i) une mesure à vide sur quelques secondes, sans exécuter d’inférence, afin d’estimer la consommation et l’utilisation des ressources dues uniquement au système et aux processus en arrière-plan ; (ii) une phase de \textit{warmup} (inférences initiales non comptabilisées) pour stabiliser le pipeline d’exécution du modèle de vision ; (iii) la répétition de chaque expérience trois fois, afin de lisser l’aléatoire, puis l’agrégation des résultats (moyenne) lors de l’analyse.

Une fois ces conditions réunies, chaque couple (\textit{hardware}, \textit{runtime}) exécute les mêmes tests d’inférence sur un jeu de données commun. Nous utilisons \texttt{coco128}, qui contient 128 images, suffisamment petit pour rester exécutable rapidement sur l’ensemble des plateformes, tout en étant assez grand pour obtenir des métriques exploitables. Pour chaque variante, l’inférence est effectuée sur l’ensemble des images et les métriques sont ensuite calculées et enregistrées au format CSV.

Enfin, la consommation électrique instantanée est mesurée de manière externe à l’aide de la prise connectée Tapo. Concrètement, la prise est insérée entre la source secteur et l’alimentation du matériel testé, ce qui permet de relever une puissance en watts au niveau système.

% Pour benchmarker sur un hard donnée (même procédure pour tous les hards), on applique trois mesures to ensure fairness 1) mesure des métriques hardwares à vide pendant quelques secondes, sans faire tourner d'inférence, pour mesurer la consommation "de base" de l'OS et app en tâches de fond, 2) avant de lancer la vraie inférence, on fait une phase de préchauffage où vont va faire quelques run pour du beurre pour stabiliser le pipeline/modèle (expliquer un peu plus en détail ça), 3) on va répéter chaque expérience trois fois, et on prendra la moyenne des résultats.

% Ensuite pour chaque hardware, on lance le test du modèle sur le hadware
% On bench avec coco128, petit, rapide, mais une centaine d'image quand même avoir des résultats exploitables et on fait les mesures en logguant les résultats dans un CSV. On a utilisé une prise Tapo pour la conso électrique, on en parlera avant, mais là juste dire que la prise on la branche entre la source de courrant et cable d'alim du hardware, enfin on branche l'allim du hard sur la prise tapo sur le courrant.

% Parles des risques et incertitudes de mesures

\section{Résultats} \label{sec:resultats}

Nous analysons maintenant les résultats obtenus. Nous distinguons deux cas d’usage : (i) un scénario confortable (4070/i9) où l’objectif est de maximiser la qualité (mAP@50) tout en minimisant la latence (E2E p95), et (ii) un scénario plus \textit{edge} (Pi/Oak) où l’on cherche à minimiser la latence tout en minimisant la puissance électrique moyenne.

Pour la 4070 et le i9, nous représentons le compromis mAP@50 (à maximiser) vs E2E p95 (à minimiser) via un front de Pareto. Un point appartient au front s’il n’est dominé par aucun autre : autrement dit, il n’existe pas de configuration simultanément plus précise (mAP@50 plus élevé) et plus rapide (E2E p95 plus faible). Les \cref{fig:pareto_pc} ne montrent donc que les configurations meilleures candidates pour ce cas d’usage (12 sur 90 pour le 4070, et 16 sur 90 pour le i9).

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_4070.png}
  \caption*{(a) 4070}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_cpu.png}
  \caption*{(b) i9}
\end{minipage}
\caption{Fronts de Pareto pour représentant la mAP@50 à maximiser, et l'E2E p95 à minimiser. Les numéros renvoient aux configurations listées dans les \cref{tab:pareto_4070,tab:pareto_cpu}.}
\label{fig:pareto_pc}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} & \textbf{VRAM} \\
\hline
1  & s 256 fp32 ORT ALL      & 0.5942 & 4.6367  & 36.12 & 117.67 & 1866.75 & 11062.20 & 358.33 \\
2  & s 256 fp16 ORT ALL      & 0.5953 & 5.2967  & 18.09 & 102.33 & 1866.75 & 10991.55 & 283.00 \\
3  & m 256 fp16 ORT ALL      & 0.6770 & 5.6300  & 38.42 & 114.00 & 1856.30 & 10683.69 & 356.33 \\
4  & m 256 fp32 ORT DISABLE  & 0.6785 & 5.8367  & 76.75 & 150.33 & 1845.47 & 10740.47 & 483.00 \\
5  & m 256 fp16 ORT BASIC    & 0.6793 & 5.8633  & 38.42 & 118.33 & 1856.31 & 10691.94 & 357.67 \\
6  & m 320 fp32 ORT ALL      & 0.7034 & 6.8000  & 76.77 & 109.33 & 1871.49 & 10751.31 & 484.33 \\
7  & m 416 fp32 ORT ALL      & 0.7452 & 8.9733  & 76.79 & 134.67 & 1843.27 & 10674.11 & 486.33 \\
8  & m 416 fp16 ORT ALL      & 0.7487 & 10.0033 & 38.44 & 122.67 & 1853.58 & 10727.44 & 356.33 \\
9  & m 512 fp32 ORT ALL      & 0.7783 & 11.5300 & 76.83 & 139.00 & 1838.39 & 10605.55 & 741.67 \\
10 & m 512 fp16 ORT ALL      & 0.7788 & 12.6167 & 38.46 & 127.33 & 1842.44 & 10652.76 & 483.67 \\
11 & m 640 fp32 ORT ALL      & 0.7800 & 16.1633 & 76.89 & 121.33 & 1481.47 & 10292.72 & 734.33 \\
12 & m 640 fp16 ORT ALL      & 0.7809 & 17.0867 & 38.48 & 121.00 & 1813.53 & 10529.27 & 483.00 \\
\hline
\end{tabular}
\caption{Configurations non dominées (front de Pareto) sur RTX 4070 Laptop pour le cas d’usage \#1. RSS, RAM sys et VRAM sont en MB ; la puissance est en W.}
\label{tab:pareto_4070}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} \\
\hline
1  & n 256 fp32 ORT ALL   & 0.4589 & 12.2367  & 10.07 & 97.33  & 866.00  & 12459.79 \\
2  & n 256 fp16 ORT ALL   & 0.4590 & 13.1567  & 5.07  & 96.67  & 866.00  & 12397.68 \\
3  & n 320 fp16 ORT ALL   & 0.5154 & 15.8967  & 5.08  & 97.67  & 866.26  & 12490.68 \\
4  & s 256 fp32 ORT ALL   & 0.5943 & 17.1333  & 36.12 & 98.33  & 884.57  & 11820.61 \\
5  & s 320 fp32 ORT ALL   & 0.6486 & 21.5167  & 36.13 & 99.00  & 888.61  & 11749.86 \\
6  & s 320 fp16 ORT ALL   & 0.6499 & 24.0067  & 18.10 & 98.00  & 891.05  & 11827.09 \\
7  & m 256 fp32 ORT ALL   & 0.6783 & 29.3333  & 76.75 & 98.33  & 981.24  & 11451.16 \\
8  & s 416 fp32 ORT ALL   & 0.6911 & 33.3133  & 36.16 & 99.33  & 925.98  & 11672.47 \\
9  & m 320 fp32 ORT ALL   & 0.7034 & 39.6133  & 76.77 & 98.56  & 996.60  & 11177.24 \\
10 & s 512 fp32 ORT ALL   & 0.7307 & 42.3200  & 36.19 & 99.00  & 937.88  & 11445.71 \\
11 & s 512 fp16 ORT ALL   & 0.7309 & 47.3567  & 18.13 & 98.22  & 944.51  & 11634.59 \\
12 & m 416 fp32 ORT ALL   & 0.7452 & 61.3667  & 76.79 & 98.83  & 1061.00 & 11123.77 \\
13 & m 512 fp32 ORT ALL   & 0.7782 & 84.3167  & 76.83 & 99.17  & 1091.08 & 11063.42 \\
14 & m 640 fp32 ORT ALL   & 0.7802 & 122.7000 & 76.89 & 125.23 & 1210.44 & 11238.51 \\
15 & m 640 fp16 ORT ALL   & 0.7805 & 132.0133 & 38.48 & 99.67  & 1267.36 & 10793.08 \\
16 & m 640 fp16 ORT BASIC & 0.7806 & 207.3733 & 38.48 & 99.03  & 1207.62 & 10769.02 \\
\hline
\end{tabular}
\caption{Configurations non dominées (front de Pareto) sur CPU i9 pour le cas d’usage \#1. La VRAM n’est pas applicable en exécution CPU.}
\label{tab:pareto_cpu}
\end{table}

Sur GPU, un fait marquant est la forte représentation de la variante \texttt{m} (10/12) et du mode \texttt{ORT ALL} (10/12), suggérant que, lorsque le budget de calcul est suffisant, les optimisations de graphe ORT au niveau maximal contribuent fortement à obtenir des points à la fois rapides et précis. Sur CPU, le front couvre davantage de tailles (\texttt{n/s/m}) — ce qui reflète un compromis plus sensible entre complexité et latence — mais \texttt{ORT ALL} domine également (15/16), avec un unique point \texttt{ORT BASIC} nettement plus lent. Les tableaux permettent aussi de constater que (i) \texttt{FP16} réduit fortement la taille du modèle (et parfois la mémoire), mais (ii) n’apporte pas systématiquement un gain de latence sur ces configurations, ce qui rejoint l’observation globale discutée plus loin.

Dans un scénario plus \textit{edge}, où l’on suppose des ressources limitées, nous considérons le compromis puissance électrique moyenne (à minimiser) vs E2E p95 (à minimiser). Les fronts obtenus sont beaucoup plus restreints : 6 configurations pour la Pi et un unique point pour l’Oak dans cet extrait, ce qui reflète des contraintes fortes (notamment la nécessité de rester sur des modèles très légers et des résolutions faibles). Dans les deux cas, le front est constitué uniquement de variantes \texttt{n} ; sur Pi on observe essentiellement \texttt{ORT ALL} (et quelques points \texttt{ORT BASIC}), et sur OAK le point retenu correspond à \texttt{S6} (6 SHAVEs).

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_pi.png}
  \caption*{(a) Raspberry Pi 4}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pareto_oak.png}
  \caption*{(b) OAK-D Pro (MyriadX)}
\end{minipage}
\caption{Fronts de Pareto pour le cas d’usage \#2 (puissance vs latence). Les numéros renvoient aux \cref{tab:pareto_pi,tab:pareto_oak}.}
\label{fig:pareto_edge}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} \\
\hline
1 & n 256 fp32 ORT ALL   & 0.4589 & 119.9933 & 10.07 & 5.8167 & 375.43 & 559.2933 \\
2 & n 256 fp16 ORT ALL   & 0.4590 & 122.5867 & 5.07  & 5.7500 & 375.43 & 556.0800 \\
3 & n 256 fp16 ORT BASIC & 0.4590 & 139.3500 & 5.07  & 5.7433 & 375.43 & 556.1000 \\
4 & n 416 fp16 ORT ALL   & 0.5801 & 288.0167 & 5.09  & 5.5900 & 375.42 & 555.6167 \\
5 & n 512 fp16 ORT ALL   & 0.6376 & 425.4933 & 5.11  & 5.1700 & 396.79 & 577.1433 \\
6 & n 512 fp16 ORT BASIC & 0.6376 & 440.4000 & 5.11  & 5.1633 & 404.66 & 585.7467 \\
\hline
\end{tabular}
\caption{Configurations non dominées (front de Pareto) sur Raspberry Pi 4 pour le cas d’usage \#2.}
\label{tab:pareto_pi}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{r l r r r r r r}
\hline
\textbf{ID} & \textbf{Label} & \textbf{mAP@50} & \textbf{p95 (ms)} & \textbf{MB} & \textbf{W} & \textbf{RSS} & \textbf{RAM sys} \\
\hline
1 & n 256 fp16 OAK S6 & 0.4567 & 72.6167 & 5.54 & 5.8667 & 386.2767 & 559.6400 \\
\hline
\end{tabular}
\caption{Configuration non dominée sur OAK-D Pro pour le cas d’usage \#2.}
\label{tab:pareto_oak}
\end{table}

Pour isoler l’effet des principaux leviers, nous analysons ensuite les métriques moyennes regroupées par facteur. Les \cref{fig:map_factors} confirment que la qualité (mAP@50) est principalement corrélée (i) à la taille du modèle (\texttt{m} $>$ \texttt{s} $>$ \texttt{n}) et (ii) à la résolution d’entrée (plus l’image est grande, plus la mAP@50 augmente), avec des tendances globalement cohérentes d’un matériel à l’autre. À l’inverse, dans nos mesures, la quantification FP16/FP32, le nombre de SHAVEs (OAK) et le niveau de fusion ORT n’ont pas montré d’impact significatif sur la mAP@50 (à l’échelle des variations observées).

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{map_per_scale.png}
  \caption*{(a) mAP@50 vs taille \texttt{n/s/m}}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{map_per_size.png}
  \caption*{(b) mAP@50 vs résolution}
\end{minipage}
\caption{Effet des principaux leviers de compilation sur la qualité (mAP@50).}
\label{fig:map_factors}
\end{figure}

Du côté des performances temporelles, les \cref{fig:e2e_factors} montrent une corrélation forte entre résolution et latence E2E : plus l’entrée est grande, plus l’inférence est lente, ce qui est attendu (plus de pixels à traiter). On notera que les barres Pi et OAK peuvent sembler proches à certains endroits car les valeurs sont agrégées : or la Pi n’exécute ici que des variantes \texttt{n}, tandis que l’OAK inclut aussi des variantes \texttt{s/m}, ce qui peut biaiser la comparaison directe si l’on ne contrôle pas la taille de modèle. Enfin, concernant les optimisations ORT, nous observons une légère dominance du mode \texttt{ALL} (et parfois \texttt{DISABLE}) par rapport à \texttt{BASIC}, ce dernier apparaissant en moyenne comme le plus lent sur ces regroupements.

\begin{figure}[h]
\centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{e2e_size.png}
  \caption*{(a) E2E p95 vs résolution}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=\linewidth]{e2e_fusion.png}
  \caption*{(b) E2E p95 vs fusion ORT}
\end{minipage}
\caption{Effet des facteurs principaux sur la latence E2E (p95).}
\label{fig:e2e_factors}
\end{figure}

Enfin, plusieurs tendances secondaires ressortent de l’ensemble des mesures : (i) FP16 apparaît très légèrement plus lent que FP32 sur certaines plateformes (écart faible), ce qui suggère que le gain attendu en calcul n’est pas toujours dominant face aux surcoûts de conversion, d’implémentation ou de pipeline selon le \textit{runtime} ; (ii) sur OAK, un nombre de SHAVEs plus faible tend à aller de pair avec une latence légèrement plus élevée ; (iii) la résolution d’entrée a un impact modéré sur la puissance moyenne (plus grand $\Rightarrow$ légèrement plus gourmand), et FP32 tend à consommer un peu plus que FP16 ; (iv) à l’inverse, les niveaux de fusion ORT et le nombre de SHAVEs montrent peu de variations de puissance dans nos relevés.

% % Faire le pont avec section précédentes, on va maintenant analyse les résultats.
% % En premier, on s'intéresse aux GPU du PC et au CPU. On considère un cas d'usage pas trop edge, ou on a du budget et de la puissance, et on s'intéresse plutôt à  maxer le map50 moyen, et minimiser l'E2E.

% Pour CPU et GPU c'est représenté avec un graph de pareto (expliquer brièvement ce que ça veut dire, le fait qu'on représente que les configurations qui domine les autres), on trouve 12 configurations alternatives (sur 90 du coup) pour le 4070, et 9 (idem) pour le CPU. fait intéressant, pour le GPU, on a 10 modèles sur 12 qui sont m, et 10 sur 12 qui sont ORT ALL.

% Pour le CPU on a un peu de tout niveau nsm, par contre 15 sur 16 sont ORT ALL. Donc pour ce cas d'usage, on semble comprendre que ORT ALL compte bcp pour trouver des modèles rapides et précis quand on a bcp de budgets.

% La, mettre les deux images cotes a cotes

% Et en dessous le tableau avec les labels et modèles correspondant

% 	Pareto_id	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	s 256 fp32 ORT ALL	0.5942	4.636667	36.12	117.666667	1866.750000	11062.200000	358.333333
% 1	2	s 256 fp16 ORT ALL	0.5953	5.296667	18.09	102.333333	1866.750000	10991.546667	283.000000
% 2	3	m 256 fp16 ORT ALL	0.6770	5.630000	38.42	114.000000	1856.300000	10683.690000	356.333333
% 3	4	m 256 fp32 ORT DISABLE	0.6785	5.836667	76.75	150.333333	1845.473333	10740.473333	483.000000
% 4	5	m 256 fp16 ORT BASIC	0.6793	5.863333	38.42	118.333333	1856.310000	10691.936667	357.666667
% 5	6	m 320 fp32 ORT ALL	0.7034	6.800000	76.77	109.333333	1871.490000	10751.306667	484.333333
% 6	7	m 416 fp32 ORT ALL	0.7452	8.973333	76.79	134.666667	1843.270000	10674.106667	486.333333
% 7	8	m 416 fp16 ORT ALL	0.7487	10.003333	38.44	122.666667	1853.580000	10727.436667	356.333333
% 8	9	m 512 fp32 ORT ALL	0.7783	11.530000	76.83	139.000000	1838.390000	10605.546667	741.666667
% 9	10	m 512 fp16 ORT ALL	0.7788	12.616667	38.46	127.333333	1842.440000	10652.756667	483.666667
% 10	11	m 640 fp32 ORT ALL	0.7800	16.163333	76.89	121.333333	1481.473333	10292.720000	734.333333
% 11	12	m 640 fp16 ORT ALL	0.7809	17.086667	38.48	121.000000	1813.526667	10529.270000	483.000000

% parler vite fait de trucs intéressants dans le tableau

% 	Pareto_id	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	n 256 fp32 ORT ALL	0.4589	12.236667	10.07	97.333333	866.000000	12459.790000	NaN
% 1	2	n 256 fp16 ORT ALL	0.4590	13.156667	5.07	96.666667	866.000000	12397.683333	NaN
% 2	3	n 320 fp16 ORT ALL	0.5154	15.896667	5.08	97.666667	866.260000	12490.683333	NaN
% 3	4	s 256 fp32 ORT ALL	0.5943	17.133333	36.12	98.333333	884.566667	11820.610000	NaN
% 4	5	s 320 fp32 ORT ALL	0.6486	21.516667	36.13	99.000000	888.610000	11749.860000	NaN
% 5	6	s 320 fp16 ORT ALL	0.6499	24.006667	18.10	98.000000	891.053333	11827.093333	NaN
% 6	7	m 256 fp32 ORT ALL	0.6783	29.333333	76.75	98.333333	981.243333	11451.163333	NaN
% 7	8	s 416 fp32 ORT ALL	0.6911	33.313333	36.16	99.333333	925.980000	11672.466667	NaN
% 8	9	m 320 fp32 ORT ALL	0.7034	39.613333	76.77	98.556667	996.603333	11177.236667	NaN
% 9	10	s 512 fp32 ORT ALL	0.7307	42.320000	36.19	99.003333	937.880000	11445.713333	NaN
% 10	11	s 512 fp16 ORT ALL	0.7309	47.356667	18.13	98.220000	944.510000	11634.593333	NaN
% 11	12	m 416 fp32 ORT ALL	0.7452	61.366667	76.79	98.833333	1061.000000	11123.773333	NaN
% 12	13	m 512 fp32 ORT ALL	0.7782	84.316667	76.83	99.166667	1091.083333	11063.420000	NaN
% 13	14	m 640 fp32 ORT ALL	0.7802	122.700000	76.89	125.233333	1210.436667	11238.510000	NaN
% 14	15	m 640 fp16 ORT ALL	0.7805	132.013333	38.48	99.666667	1267.356667	10793.080000	NaN
% 15	16	m 640 fp16 ORT BASIC	0.7806	207.373333	38.48	99.026667	1207.616667	10769.016667	NaN



% Ensuite deuxième cas d'usage, plus edge, on a un des ressources limitées donc on veut minimiser l'E2E tout en minimisant la conso élec, par exemple, là on voit qu'on a que 6 modèles pour Pi, et 1 seul pour Oak. dans les deux cas, c'est que des modèles n, avec ort basic ou all sur pi, et 6 shaves sur la oak.

% mettre les deux graphes pareto

% et en dessous tableau ...

% Pareto_id_power	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	n 256 fp32 ORT ALL	0.4589	119.993333	10.07	5.816667	375.43	559.293333	NaN
% 1	2	n 256 fp16 ORT ALL	0.4590	122.586667	5.07	5.750000	375.43	556.080000	NaN
% 2	3	n 256 fp16 ORT BASIC	0.4590	139.350000	5.07	5.743333	375.43	556.100000	NaN
% 3	4	n 416 fp16 ORT ALL	0.5801	288.016667	5.09	5.590000	375.42	555.616667	NaN
% 4	5	n 512 fp16 ORT ALL	0.6376	425.493333	5.11	5.170000	396.79	577.143333	NaN
% 5	6	n 512 fp16 ORT BASIC	0.6376	440.400000	5.11	5.163333	404.66	585.746667	NaN


% 	Pareto_id_power	Label	mAP50	Latency_p95	Size_MB	Power_W_Mean	RAM_RSS_MB_Mean	RAM_Sys_Used_MB_Mean	VRAM_Used_MB_Mean
% 0	1	n 256 fp16 OAK S6	0.4567	72.616667	5.54	5.866667	386.276667	559.64	NaN

% parler aussi vite fait des tableau

% Ensuite, pour l'analyse de l'impact des performances des optimisations, quand on étudie le bargraphe du map50 moyen par quantification, on constate que la map est systématiquement corrélée avec la taille du modèle, plus il est gros, plus c'est élevé, et d'ailleurs ça varie pas bcp d'un hard à l'autre par contre.

% pour la map moy par input size, mêmes observations, plus elle est élevée, plus la map augmente.

% la mettre les deux graphes map/quantif et map/input size cote cote

% pour la quantification, le nb de shaves et la fusion ort, on a observée aucun impact significatif sur le map.

% pour la E2E par input size pareil c'est fortement corrélé, plus c'est grand, plus c'est lent. on précise sur le graphe que oak et pi semblent aux mêmes niveaux parce qu'on a moyenné les vvaleurs mais que pi a eu que du nano alors que oak a eu du m et s

% pour le E2E par fusion, on observe une légère dominance du mode all, et disable, et etonnament basic le plus lent.

% la mettre les deux graphes cote cote

% pour fp16/32, expliquer qu'on a observé (pas de graphe ici) que fp16 est très légèrement plus lent que 32.

% que pour les shaves, moins de shaves semble aller de pair avec légèrement plus lent

% que l'input size à un impact très modéré sur la conso en watt, avec une plus grande taille indiquant généralement une plus grosse conso

% idem pour fp16/32, avec fp32 plus gourmand

% pas de différences entre les shaves sur la conso et peu de variation entre les niveaux de fusions et la conso.

\section{Analyse et conclusion} \label{sec:analyse_et_conclusion}


% \section{Description du système}
% \subsection{Identification des besoins capteurs} %Identification of the sensors, their sources of uncertainty, and working ranges
% \subsection{Caracteristiques des capteurs }
% \subsection{Sources d'incertitude}
% \section{Sensor data pre-processing stage}
% \subsection{Architecture du Pipeline d'Inférence sur OAK-D}
% \section{NN architecture and computation graph description (il faut parler de yolo ici?)}
% \subsection{YOLO11}
% \section{Model optimisation process}
% \subsection{Ingestion et Standardisation des Données}
% \subsection{Entraînement et Transfer Learning}
% \subsection{Génération Automatisée de Variantes}
% \subsection{Compilation Spécifique au Matériel}
% \section{Résultat} %Benchmark results
% \subsection{Criteres d'évaluation}
% \subsection{résultats par rapport au méteriel}
% \subsection{résultat par rapport au modèle }
% \section{Github repo and documentation}
\end{document}
